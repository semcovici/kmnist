{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import keras\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard, CSVLogger\n",
    "import pandas as pd\n",
    "import gc\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "sys.path.append('../src')\n",
    "from data.utils import load\n",
    "from models.create_models import create_cnn_model, create_fcnn_model\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/\"\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "\n",
    "PATH_MODELS = \"../models/\"\n",
    "PATH_LOGS = \"../logs/\"\n",
    "RESULTS_PATH = \"../results/\"\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples, 10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "X_train = load(data_dir + 'kmnist-train-imgs.npz')\n",
    "X_test = load(data_dir + 'kmnist-test-imgs.npz')\n",
    "y_train = load(data_dir + 'kmnist-train-labels.npz')\n",
    "y_test = load(data_dir + 'kmnist-test-labels.npz')\n",
    "\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    \n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('{} train samples, {} test samples'.format(len(X_train), len(X_test)))\n",
    "\n",
    "# coleta dos valores unicos e das contagens\n",
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "\n",
    "num_classes = len(unique_train) if list(unique_train) == list(unique_test) else None\n",
    "\n",
    "\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criar conjunto aumentado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie o gerador de dados com data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=False,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Supomos que você tenha suas imagens de treino em x_train e labels em y_train\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Defina o número de amostras aumentadas que você deseja criar\n",
    "num_augmented_samples = len(X_train) * 10  # por exemplo, 10 vezes o conjunto original\n",
    "\n",
    "# Inicialize arrays para armazenar os dados aumentados\n",
    "X_train_augmented = np.zeros((num_augmented_samples, *X_train.shape[1:]), dtype=np.float32)\n",
    "y_train_augmented = np.zeros((num_augmented_samples, *y_train.shape[1:]), dtype=np.float32)\n",
    "\n",
    "# Gere os dados aumentados\n",
    "i = 0\n",
    "for x_batch, y_batch in datagen.flow(X_train, y_train, batch_size=BATCH_SIZE):\n",
    "    if i >= num_augmented_samples:\n",
    "        break\n",
    "    batch_size_aux = x_batch.shape[0]\n",
    "    X_train_augmented[i:i+batch_size_aux] = x_batch\n",
    "    y_train_augmented[i:i+batch_size_aux] = y_batch\n",
    "    i += batch_size_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num class train:  10\n",
      "Num class test:  10\n",
      "### Treino ###\n",
      "classe 0: 6000 observações\n",
      "classe 1: 6000 observações\n",
      "classe 2: 6000 observações\n",
      "classe 3: 6000 observações\n",
      "classe 4: 6000 observações\n",
      "classe 5: 6000 observações\n",
      "classe 6: 6000 observações\n",
      "classe 7: 6000 observações\n",
      "classe 8: 6000 observações\n",
      "classe 9: 6000 observações\n",
      "\n",
      "### Teste ###\n",
      "classe 0: 1000 observações\n",
      "classe 1: 1000 observações\n",
      "classe 2: 1000 observações\n",
      "classe 3: 1000 observações\n",
      "classe 4: 1000 observações\n",
      "classe 5: 1000 observações\n",
      "classe 6: 1000 observações\n",
      "classe 7: 1000 observações\n",
      "classe 8: 1000 observações\n",
      "classe 9: 1000 observações\n"
     ]
    }
   ],
   "source": [
    "print(\"Num class train: \", len(unique_train))\n",
    "print(\"Num class test: \", len(unique_test))\n",
    "\n",
    "\n",
    "print('### Treino ###')\n",
    "for i in range(len(unique_train)): print(f'classe {unique_train[i]}: {counts_train[i]} observações')\n",
    "print('\\n### Teste ###')\n",
    "for i in range(len(unique_test)): print(f'classe {unique_test[i]}: {counts_test[i]} observações')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS =[\n",
    "        'accuracy',\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc'),\n",
    "        keras.metrics.TruePositives(name='tp'),\n",
    "        keras.metrics.TrueNegatives(name='tn'),\n",
    "        keras.metrics.FalsePositives(name='fp'),\n",
    "        keras.metrics.FalseNegatives(name='fn')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "### CNN Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks do modelo benchmark\n",
    "callbacks_benchmark = [\n",
    "    ModelCheckpoint(PATH_MODELS + f'best_model_CNN_benchmark.keras', save_best_only=True, monitor='val_loss'),\n",
    "    TensorBoard(log_dir=PATH_LOGS),\n",
    "    CSVLogger(PATH_MODELS + f'training_log_CNN_benchmark.csv')\n",
    "]\n",
    "\n",
    "# limpa sessao do keras\n",
    "keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "# cria modelo\n",
    "model_benchmark = create_cnn_model(\n",
    "    input_shape=input_shape,\n",
    "    num_classes= num_classes,\n",
    "    metrics= METRICS\n",
    ")\n",
    "\n",
    "# treina modelo\n",
    "history_benchmark = model_benchmark.fit(X_train, y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=12,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test),\n",
    "          callbacks = callbacks_benchmark\n",
    "          )\n",
    "\n",
    "# realiza predicao\n",
    "train_pred_proba = model_benchmark.predict(X_train)\n",
    "test_pred_proba = model_benchmark.predict(X_test)\n",
    "\n",
    "# salva predicao no treino\n",
    "test_results_benchmark = pd.DataFrame({\n",
    "    \"y_test\": y_test.argmax(axis=1),\n",
    "    \"y_test_pred\": np.argmax(test_pred_proba, axis =1),\n",
    "    \"y_test_pred_proba\": list(test_pred_proba)\n",
    "    \n",
    "})\n",
    "test_results_benchmark.to_csv(RESULTS_PATH + 'test/cnn_benchmark_test_results.csv', index = False)\n",
    "\n",
    "# salva predicao no teste\n",
    "train_results_benchmark = pd.DataFrame({\n",
    "    \"y_train\": y_train.argmax(axis=1),\n",
    "    \"y_train_pred\": np.argmax(train_pred_proba, axis =1),\n",
    "    \"y_train_pred_proba\": list(train_pred_proba)\n",
    "})\n",
    "train_results_benchmark.to_csv(RESULTS_PATH + 'train/cnn_benchmark_train_results.csv', index = False)\n",
    "\n",
    "json.dump(history_benchmark.history, open(RESULTS_PATH + \"history/benchmark_cnn_history.json\", \"w\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extraindo os pesos da primeira camada convolucional\n",
    "filters, biases = model_benchmark.layers[0].get_weights()\n",
    "n_filters = filters.shape[-1]\n",
    "\n",
    "# Normalizando os filtros para a faixa [0, 1]\n",
    "filters_min = filters.min()\n",
    "filters_max = filters.max()\n",
    "filters = (filters - filters_min) / (filters_max - filters_min)\n",
    "\n",
    "# Visualizando os filtros\n",
    "fig, axs = plt.subplots(1, n_filters, figsize=(20, 20))\n",
    "for i in range(n_filters):\n",
    "    f = filters[:, :, :, i]\n",
    "    axs[i].imshow(f[:, :, 0], cmap='gray')\n",
    "    axs[i].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Com Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks do modelo benchmark\n",
    "callbacks_benchmark_aug = [\n",
    "    ModelCheckpoint(PATH_MODELS + f'best_model_CNN_aug_benchmark.keras', save_best_only=True, monitor='val_loss'),\n",
    "    TensorBoard(log_dir=PATH_LOGS),\n",
    "    CSVLogger(PATH_MODELS + f'training_log_CNN_aug_benchmark.csv')\n",
    "]\n",
    "\n",
    "# limpa sessao do keras\n",
    "keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "# cria modelo\n",
    "model_benchmark_aug = create_cnn_model(\n",
    "    input_shape=input_shape,\n",
    "    num_classes= num_classes,\n",
    "    metrics= METRICS\n",
    ")\n",
    "\n",
    "# treina modelo\n",
    "history_benchmark_aug = model_benchmark_aug.fit(X_train_augmented, y_train_augmented,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=12,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test),\n",
    "          callbacks = callbacks_benchmark_aug\n",
    "          )\n",
    "\n",
    "# realiza predicao\n",
    "train_pred_proba = model_benchmark_aug.predict(X_train)\n",
    "test_pred_proba = model_benchmark_aug.predict(X_test)\n",
    "\n",
    "# salva predicao no treino\n",
    "test_results_benchmark_aug = pd.DataFrame({\n",
    "    \"y_test\": y_test.argmax(axis=1),\n",
    "    \"y_test_pred\": np.argmax(test_pred_proba, axis =1),\n",
    "    \"y_test_pred_proba\": list(test_pred_proba)\n",
    "    \n",
    "})\n",
    "test_results_benchmark_aug.to_csv(RESULTS_PATH + 'test/cnn_benchmark_aug_test_results.csv', index = False)\n",
    "\n",
    "# salva predicao no teste\n",
    "train_results_benchmark_aug = pd.DataFrame({\n",
    "    \"y_train\": y_train.argmax(axis=1),\n",
    "    \"y_train_pred\": np.argmax(train_pred_proba, axis =1),\n",
    "    \"y_train_pred_proba\": list(train_pred_proba)\n",
    "})\n",
    "train_results_benchmark_aug.to_csv(RESULTS_PATH + 'train/cnn_benchmark_aug_train_results.csv', index = False)\n",
    "\n",
    "json.dump(history_benchmark_aug.history, open(RESULTS_PATH + \"history/benchmark_aug_cnn_history.json\", \"w\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks cnn\n",
    "callbacks_cnn = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10),\n",
    "    ModelCheckpoint(PATH_MODELS + f'best_model_CNN.keras', save_best_only=True, monitor='val_loss'),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5),\n",
    "    TensorBoard(log_dir=PATH_LOGS),\n",
    "    CSVLogger(PATH_MODELS + f'training_log_CNN.csv')\n",
    "]\n",
    "\n",
    "# limpa sessao do keras\n",
    "keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "# cria modelo\n",
    "model_cnn = create_cnn_model(\n",
    "    input_shape=input_shape,\n",
    "    num_classes= num_classes,\n",
    "    metrics = METRICS\n",
    ")\n",
    "\n",
    "# treina modelo\n",
    "history_cnn = model_cnn.fit(X_train, y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test),\n",
    "          callbacks=callbacks_cnn\n",
    "          )\n",
    "\n",
    "\n",
    "# realiza predicao\n",
    "train_pred_proba = model_cnn.predict(X_train)\n",
    "test_pred_proba = model_cnn.predict(X_test)\n",
    "\n",
    "# salva predicao no treino\n",
    "test_results_cnn = pd.DataFrame({\n",
    "    \"y_test\": y_test.argmax(axis=1),\n",
    "    \"y_test_pred\": np.argmax(test_pred_proba, axis =1),\n",
    "    \"y_test_pred_proba\": list(test_pred_proba)\n",
    "    \n",
    "})\n",
    "test_results_cnn.to_csv(RESULTS_PATH + 'test/cnn_test_results.csv', index = False)\n",
    "\n",
    "# salva predicao no teste\n",
    "train_results_cnn = pd.DataFrame({\n",
    "    \"y_train\": y_train.argmax(axis=1),\n",
    "    \"y_train_pred\": np.argmax(train_pred_proba, axis =1),\n",
    "    \"y_train_pred_proba\": list(train_pred_proba)\n",
    "})\n",
    "train_results_cnn.to_csv(RESULTS_PATH + 'train/cnn_train_results.csv', index = False)\n",
    "\n",
    "\n",
    "json.dump(history_cnn.history, open(RESULTS_PATH + \"history/cnn_history.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sem pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks cnn\n",
    "callbacks_cnn_wo_poll = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10),\n",
    "    ModelCheckpoint(PATH_MODELS + f'best_model_CNN_without_polling.keras', save_best_only=True, monitor='val_loss'),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5),\n",
    "    TensorBoard(log_dir=PATH_LOGS),\n",
    "    CSVLogger(PATH_MODELS + f'training_log_CNN_without_polling.csv')\n",
    "]\n",
    "\n",
    "# limpa sessao do keras\n",
    "keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "# cria modelo\n",
    "model_cnn_wo_poll = create_cnn_model(\n",
    "    input_shape=input_shape,\n",
    "    num_classes= num_classes,\n",
    "    metrics = METRICS,\n",
    "    polling = False\n",
    ")\n",
    "\n",
    "# treina modelo\n",
    "history_cnn_wo_poll = model_cnn_wo_poll.fit(X_train, y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test),\n",
    "          callbacks=callbacks_cnn_wo_poll\n",
    "          )\n",
    "\n",
    "\n",
    "# realiza predicao\n",
    "train_pred_proba = model_cnn_wo_poll.predict(X_train)\n",
    "test_pred_proba = model_cnn_wo_poll.predict(X_test)\n",
    "\n",
    "# salva predicao no treino\n",
    "test_results_cnn_wo_poll = pd.DataFrame({\n",
    "    \"y_test\": y_test.argmax(axis=1),\n",
    "    \"y_test_pred\": np.argmax(test_pred_proba, axis =1),\n",
    "    \"y_test_pred_proba\": list(test_pred_proba)\n",
    "    \n",
    "})\n",
    "test_results_cnn_wo_poll.to_csv(RESULTS_PATH + 'test/cnn_without_polling_test_results.csv', index = False)\n",
    "\n",
    "# salva predicao no teste\n",
    "train_results_cnn_wo_poll = pd.DataFrame({\n",
    "    \"y_train\": y_train.argmax(axis=1),\n",
    "    \"y_train_pred\": np.argmax(train_pred_proba, axis =1),\n",
    "    \"y_train_pred_proba\": list(train_pred_proba)\n",
    "})\n",
    "train_results_cnn_wo_poll.to_csv(RESULTS_PATH + 'train/cnn_without_polling_train_results.csv', index = False)\n",
    "\n",
    "\n",
    "json.dump(history_cnn_wo_poll.history, open(RESULTS_PATH + \"history/cnn_without_polling_history.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Com data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-04 20:51:08.658693: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1881600000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 9ms/step - accuracy: 0.2008 - auc: 0.6433 - fn: 300140.6562 - fp: 1.5010 - loss: 2.2286 - precision: 0.1924 - recall: 3.3718e-05 - tn: 2701438.0000 - tp: 19.2922 - val_accuracy: 0.4908 - val_auc: 0.8069 - val_fn: 9971.0000 - val_fp: 2.0000 - val_loss: 1.8614 - val_precision: 0.9355 - val_recall: 0.0029 - val_tn: 89998.0000 - val_tp: 29.0000 - learning_rate: 0.0010\n",
      "Epoch 2/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 9ms/step - accuracy: 0.4373 - auc: 0.8260 - fn: 277099.0938 - fp: 4787.3828 - loss: 1.7452 - precision: 0.8457 - recall: 0.0583 - tn: 2696652.2500 - tp: 23060.8398 - val_accuracy: 0.5380 - val_auc: 0.8596 - val_fn: 7899.0000 - val_fp: 221.0000 - val_loss: 1.5138 - val_precision: 0.9048 - val_recall: 0.2101 - val_tn: 89779.0000 - val_tp: 2101.0000 - learning_rate: 0.0010\n",
      "Epoch 3/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 9ms/step - accuracy: 0.5202 - auc: 0.8729 - fn: 224404.9688 - fp: 19775.5078 - loss: 1.4707 - precision: 0.7935 - recall: 0.2430 - tn: 2681664.0000 - tp: 75754.9766 - val_accuracy: 0.5753 - val_auc: 0.8933 - val_fn: 6841.0000 - val_fp: 443.0000 - val_loss: 1.3424 - val_precision: 0.8770 - val_recall: 0.3159 - val_tn: 89557.0000 - val_tp: 3159.0000 - learning_rate: 0.0010\n",
      "Epoch 4/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 10ms/step - accuracy: 0.5634 - auc: 0.8956 - fn: 199552.0469 - fp: 26485.0234 - loss: 1.3378 - precision: 0.7914 - recall: 0.3298 - tn: 2674954.5000 - tp: 100607.8906 - val_accuracy: 0.6127 - val_auc: 0.9116 - val_fn: 6221.0000 - val_fp: 577.0000 - val_loss: 1.2314 - val_precision: 0.8675 - val_recall: 0.3779 - val_tn: 89423.0000 - val_tp: 3779.0000 - learning_rate: 0.0010\n",
      "Epoch 5/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 11ms/step - accuracy: 0.5919 - auc: 0.9082 - fn: 184245.2656 - fp: 29561.7383 - loss: 1.2552 - precision: 0.7970 - recall: 0.3829 - tn: 2671877.7500 - tp: 115914.6719 - val_accuracy: 0.6385 - val_auc: 0.9231 - val_fn: 5699.0000 - val_fp: 664.0000 - val_loss: 1.1506 - val_precision: 0.8663 - val_recall: 0.4301 - val_tn: 89336.0000 - val_tp: 4301.0000 - learning_rate: 0.0010\n",
      "Epoch 6/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.6128 - auc: 0.9170 - fn: 172987.0469 - fp: 31471.2480 - loss: 1.1938 - precision: 0.8013 - recall: 0.4210 - tn: 2669968.2500 - tp: 127172.9062 - val_accuracy: 0.6551 - val_auc: 0.9312 - val_fn: 5340.0000 - val_fp: 713.0000 - val_loss: 1.0907 - val_precision: 0.8673 - val_recall: 0.4660 - val_tn: 89287.0000 - val_tp: 4660.0000 - learning_rate: 0.0010\n",
      "Epoch 7/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.6280 - auc: 0.9230 - fn: 164961.3125 - fp: 32853.2695 - loss: 1.1495 - precision: 0.8039 - recall: 0.4481 - tn: 2668586.2500 - tp: 135198.6406 - val_accuracy: 0.6705 - val_auc: 0.9371 - val_fn: 5037.0000 - val_fp: 729.0000 - val_loss: 1.0432 - val_precision: 0.8719 - val_recall: 0.4963 - val_tn: 89271.0000 - val_tp: 4963.0000 - learning_rate: 0.0010\n",
      "Epoch 8/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 10ms/step - accuracy: 0.6417 - auc: 0.9283 - fn: 157581.3906 - fp: 33062.8477 - loss: 1.1083 - precision: 0.8116 - recall: 0.4732 - tn: 2668376.7500 - tp: 142578.5469 - val_accuracy: 0.6840 - val_auc: 0.9415 - val_fn: 4804.0000 - val_fp: 755.0000 - val_loss: 1.0050 - val_precision: 0.8731 - val_recall: 0.5196 - val_tn: 89245.0000 - val_tp: 5196.0000 - learning_rate: 0.0010\n",
      "Epoch 9/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 10ms/step - accuracy: 0.6528 - auc: 0.9322 - fn: 152349.0469 - fp: 33662.1250 - loss: 1.0771 - precision: 0.8141 - recall: 0.4912 - tn: 2667777.5000 - tp: 147810.8906 - val_accuracy: 0.6927 - val_auc: 0.9453 - val_fn: 4646.0000 - val_fp: 773.0000 - val_loss: 0.9723 - val_precision: 0.8738 - val_recall: 0.5354 - val_tn: 89227.0000 - val_tp: 5354.0000 - learning_rate: 0.0010\n",
      "Epoch 10/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.6624 - auc: 0.9358 - fn: 147578.0938 - fp: 33849.9180 - loss: 1.0480 - precision: 0.8180 - recall: 0.5072 - tn: 2667589.5000 - tp: 152581.8438 - val_accuracy: 0.7003 - val_auc: 0.9484 - val_fn: 4447.0000 - val_fp: 778.0000 - val_loss: 0.9438 - val_precision: 0.8771 - val_recall: 0.5553 - val_tn: 89222.0000 - val_tp: 5553.0000 - learning_rate: 0.0010\n",
      "Epoch 11/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 11ms/step - accuracy: 0.6707 - auc: 0.9389 - fn: 143309.8125 - fp: 34128.5156 - loss: 1.0218 - precision: 0.8209 - recall: 0.5216 - tn: 2667311.0000 - tp: 156850.1406 - val_accuracy: 0.7078 - val_auc: 0.9512 - val_fn: 4294.0000 - val_fp: 763.0000 - val_loss: 0.9172 - val_precision: 0.8821 - val_recall: 0.5706 - val_tn: 89237.0000 - val_tp: 5706.0000 - learning_rate: 0.0010\n",
      "Epoch 12/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.6788 - auc: 0.9416 - fn: 139603.9375 - fp: 34104.5117 - loss: 0.9987 - precision: 0.8246 - recall: 0.5339 - tn: 2667335.0000 - tp: 160556.0000 - val_accuracy: 0.7152 - val_auc: 0.9534 - val_fn: 4166.0000 - val_fp: 770.0000 - val_loss: 0.8960 - val_precision: 0.8834 - val_recall: 0.5834 - val_tn: 89230.0000 - val_tp: 5834.0000 - learning_rate: 0.0010\n",
      "Epoch 13/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 12ms/step - accuracy: 0.6860 - auc: 0.9441 - fn: 135926.8438 - fp: 34046.1758 - loss: 0.9766 - precision: 0.8284 - recall: 0.5465 - tn: 2667393.2500 - tp: 164233.0938 - val_accuracy: 0.7240 - val_auc: 0.9556 - val_fn: 4040.0000 - val_fp: 766.0000 - val_loss: 0.8743 - val_precision: 0.8861 - val_recall: 0.5960 - val_tn: 89234.0000 - val_tp: 5960.0000 - learning_rate: 0.0010\n",
      "Epoch 14/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 12ms/step - accuracy: 0.6927 - auc: 0.9464 - fn: 132659.6562 - fp: 33857.4297 - loss: 0.9562 - precision: 0.8320 - recall: 0.5576 - tn: 2667582.0000 - tp: 167500.2969 - val_accuracy: 0.7308 - val_auc: 0.9576 - val_fn: 3920.0000 - val_fp: 762.0000 - val_loss: 0.8544 - val_precision: 0.8886 - val_recall: 0.6080 - val_tn: 89238.0000 - val_tp: 6080.0000 - learning_rate: 0.0010\n",
      "Epoch 15/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 12ms/step - accuracy: 0.6998 - auc: 0.9484 - fn: 129789.6172 - fp: 33514.0742 - loss: 0.9369 - precision: 0.8356 - recall: 0.5668 - tn: 2667925.5000 - tp: 170370.3281 - val_accuracy: 0.7370 - val_auc: 0.9594 - val_fn: 3832.0000 - val_fp: 754.0000 - val_loss: 0.8359 - val_precision: 0.8911 - val_recall: 0.6168 - val_tn: 89246.0000 - val_tp: 6168.0000 - learning_rate: 0.0010\n",
      "Epoch 16/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 12ms/step - accuracy: 0.7053 - auc: 0.9504 - fn: 126874.2656 - fp: 33391.5469 - loss: 0.9182 - precision: 0.8382 - recall: 0.5763 - tn: 2668048.0000 - tp: 173285.6719 - val_accuracy: 0.7417 - val_auc: 0.9609 - val_fn: 3757.0000 - val_fp: 758.0000 - val_loss: 0.8204 - val_precision: 0.8917 - val_recall: 0.6243 - val_tn: 89242.0000 - val_tp: 6243.0000 - learning_rate: 0.0010\n",
      "Epoch 17/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.7124 - auc: 0.9523 - fn: 123865.3359 - fp: 33082.0312 - loss: 0.8995 - precision: 0.8420 - recall: 0.5867 - tn: 2668357.5000 - tp: 176294.6094 - val_accuracy: 0.7473 - val_auc: 0.9625 - val_fn: 3671.0000 - val_fp: 756.0000 - val_loss: 0.8029 - val_precision: 0.8933 - val_recall: 0.6329 - val_tn: 89244.0000 - val_tp: 6329.0000 - learning_rate: 0.0010\n",
      "Epoch 18/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 11ms/step - accuracy: 0.7181 - auc: 0.9542 - fn: 121524.2266 - fp: 32779.3125 - loss: 0.8816 - precision: 0.8449 - recall: 0.5945 - tn: 2668660.2500 - tp: 178635.7188 - val_accuracy: 0.7506 - val_auc: 0.9637 - val_fn: 3592.0000 - val_fp: 756.0000 - val_loss: 0.7892 - val_precision: 0.8945 - val_recall: 0.6408 - val_tn: 89244.0000 - val_tp: 6408.0000 - learning_rate: 0.0010\n",
      "Epoch 19/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 12ms/step - accuracy: 0.7244 - auc: 0.9559 - fn: 118608.7422 - fp: 32292.1973 - loss: 0.8638 - precision: 0.8488 - recall: 0.6040 - tn: 2669147.2500 - tp: 181551.2031 - val_accuracy: 0.7569 - val_auc: 0.9651 - val_fn: 3517.0000 - val_fp: 757.0000 - val_loss: 0.7731 - val_precision: 0.8954 - val_recall: 0.6483 - val_tn: 89243.0000 - val_tp: 6483.0000 - learning_rate: 0.0010\n",
      "Epoch 20/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 16ms/step - accuracy: 0.7304 - auc: 0.9577 - fn: 116227.5469 - fp: 32116.1562 - loss: 0.8462 - precision: 0.8512 - recall: 0.6122 - tn: 2669323.2500 - tp: 183932.3906 - val_accuracy: 0.7594 - val_auc: 0.9664 - val_fn: 3440.0000 - val_fp: 760.0000 - val_loss: 0.7583 - val_precision: 0.8962 - val_recall: 0.6560 - val_tn: 89240.0000 - val_tp: 6560.0000 - learning_rate: 0.0010\n",
      "Epoch 21/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 15ms/step - accuracy: 0.7350 - auc: 0.9591 - fn: 113889.4453 - fp: 31854.7422 - loss: 0.8314 - precision: 0.8538 - recall: 0.6195 - tn: 2669584.7500 - tp: 186270.5000 - val_accuracy: 0.7644 - val_auc: 0.9676 - val_fn: 3368.0000 - val_fp: 745.0000 - val_loss: 0.7440 - val_precision: 0.8990 - val_recall: 0.6632 - val_tn: 89255.0000 - val_tp: 6632.0000 - learning_rate: 0.0010\n",
      "Epoch 22/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 12ms/step - accuracy: 0.7394 - auc: 0.9605 - fn: 111633.2500 - fp: 31212.3555 - loss: 0.8162 - precision: 0.8577 - recall: 0.6274 - tn: 2670227.2500 - tp: 188526.6875 - val_accuracy: 0.7673 - val_auc: 0.9686 - val_fn: 3287.0000 - val_fp: 752.0000 - val_loss: 0.7311 - val_precision: 0.8993 - val_recall: 0.6713 - val_tn: 89248.0000 - val_tp: 6713.0000 - learning_rate: 0.0010\n",
      "Epoch 23/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 12ms/step - accuracy: 0.7462 - auc: 0.9623 - fn: 109033.2344 - fp: 30962.1328 - loss: 0.7972 - precision: 0.8606 - recall: 0.6363 - tn: 2670477.5000 - tp: 191126.7031 - val_accuracy: 0.7719 - val_auc: 0.9696 - val_fn: 3215.0000 - val_fp: 744.0000 - val_loss: 0.7188 - val_precision: 0.9012 - val_recall: 0.6785 - val_tn: 89256.0000 - val_tp: 6785.0000 - learning_rate: 0.0010\n",
      "Epoch 24/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 13ms/step - accuracy: 0.7509 - auc: 0.9637 - fn: 106804.0938 - fp: 30506.2656 - loss: 0.7825 - precision: 0.8635 - recall: 0.6431 - tn: 2670933.2500 - tp: 193355.8594 - val_accuracy: 0.7754 - val_auc: 0.9706 - val_fn: 3160.0000 - val_fp: 726.0000 - val_loss: 0.7073 - val_precision: 0.9040 - val_recall: 0.6840 - val_tn: 89274.0000 - val_tp: 6840.0000 - learning_rate: 0.0010\n",
      "Epoch 25/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 14ms/step - accuracy: 0.7565 - auc: 0.9649 - fn: 104298.7188 - fp: 30080.6465 - loss: 0.7673 - precision: 0.8671 - recall: 0.6517 - tn: 2671358.7500 - tp: 195861.2344 - val_accuracy: 0.7786 - val_auc: 0.9715 - val_fn: 3099.0000 - val_fp: 732.0000 - val_loss: 0.6955 - val_precision: 0.9041 - val_recall: 0.6901 - val_tn: 89268.0000 - val_tp: 6901.0000 - learning_rate: 0.0010\n",
      "Epoch 26/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 15ms/step - accuracy: 0.7613 - auc: 0.9662 - fn: 102283.7422 - fp: 29966.1055 - loss: 0.7531 - precision: 0.8682 - recall: 0.6587 - tn: 2671473.5000 - tp: 197876.2031 - val_accuracy: 0.7825 - val_auc: 0.9725 - val_fn: 3052.0000 - val_fp: 714.0000 - val_loss: 0.6826 - val_precision: 0.9068 - val_recall: 0.6948 - val_tn: 89286.0000 - val_tp: 6948.0000 - learning_rate: 0.0010\n",
      "Epoch 27/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 14ms/step - accuracy: 0.7666 - auc: 0.9674 - fn: 99954.6719 - fp: 29427.2500 - loss: 0.7384 - precision: 0.8719 - recall: 0.6663 - tn: 2672012.2500 - tp: 200205.2656 - val_accuracy: 0.7865 - val_auc: 0.9732 - val_fn: 2993.0000 - val_fp: 723.0000 - val_loss: 0.6739 - val_precision: 0.9065 - val_recall: 0.7007 - val_tn: 89277.0000 - val_tp: 7007.0000 - learning_rate: 0.0010\n",
      "Epoch 28/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 16ms/step - accuracy: 0.7713 - auc: 0.9687 - fn: 97888.1875 - fp: 29012.4512 - loss: 0.7239 - precision: 0.8744 - recall: 0.6730 - tn: 2672427.0000 - tp: 202271.7500 - val_accuracy: 0.7902 - val_auc: 0.9741 - val_fn: 2944.0000 - val_fp: 711.0000 - val_loss: 0.6622 - val_precision: 0.9085 - val_recall: 0.7056 - val_tn: 89289.0000 - val_tp: 7056.0000 - learning_rate: 0.0010\n",
      "Epoch 29/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 13ms/step - accuracy: 0.7766 - auc: 0.9698 - fn: 95739.1562 - fp: 28764.6133 - loss: 0.7094 - precision: 0.8765 - recall: 0.6804 - tn: 2672675.0000 - tp: 204420.7812 - val_accuracy: 0.7937 - val_auc: 0.9748 - val_fn: 2878.0000 - val_fp: 718.0000 - val_loss: 0.6518 - val_precision: 0.9084 - val_recall: 0.7122 - val_tn: 89282.0000 - val_tp: 7122.0000 - learning_rate: 0.0010\n",
      "Epoch 30/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 12ms/step - accuracy: 0.7810 - auc: 0.9709 - fn: 93756.3047 - fp: 28282.4941 - loss: 0.6959 - precision: 0.8796 - recall: 0.6876 - tn: 2673157.0000 - tp: 206403.6406 - val_accuracy: 0.7975 - val_auc: 0.9756 - val_fn: 2840.0000 - val_fp: 711.0000 - val_loss: 0.6407 - val_precision: 0.9097 - val_recall: 0.7160 - val_tn: 89289.0000 - val_tp: 7160.0000 - learning_rate: 0.0010\n",
      "Epoch 31/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 11ms/step - accuracy: 0.7856 - auc: 0.9718 - fn: 91726.9531 - fp: 27946.3711 - loss: 0.6839 - precision: 0.8817 - recall: 0.6938 - tn: 2673493.2500 - tp: 208433.0000 - val_accuracy: 0.7978 - val_auc: 0.9760 - val_fn: 2789.0000 - val_fp: 724.0000 - val_loss: 0.6340 - val_precision: 0.9088 - val_recall: 0.7211 - val_tn: 89276.0000 - val_tp: 7211.0000 - learning_rate: 0.0010\n",
      "Epoch 32/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 12ms/step - accuracy: 0.7879 - auc: 0.9727 - fn: 90269.1016 - fp: 27662.8613 - loss: 0.6722 - precision: 0.8838 - recall: 0.6990 - tn: 2673776.7500 - tp: 209890.8438 - val_accuracy: 0.8000 - val_auc: 0.9766 - val_fn: 2744.0000 - val_fp: 722.0000 - val_loss: 0.6252 - val_precision: 0.9095 - val_recall: 0.7256 - val_tn: 89278.0000 - val_tp: 7256.0000 - learning_rate: 0.0010\n",
      "Epoch 33/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 15ms/step - accuracy: 0.7933 - auc: 0.9739 - fn: 88239.7969 - fp: 27244.2109 - loss: 0.6584 - precision: 0.8864 - recall: 0.7056 - tn: 2674195.2500 - tp: 211920.1406 - val_accuracy: 0.8024 - val_auc: 0.9773 - val_fn: 2702.0000 - val_fp: 715.0000 - val_loss: 0.6147 - val_precision: 0.9108 - val_recall: 0.7298 - val_tn: 89285.0000 - val_tp: 7298.0000 - learning_rate: 0.0010\n",
      "Epoch 34/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 16ms/step - accuracy: 0.7966 - auc: 0.9746 - fn: 86497.0703 - fp: 27093.9062 - loss: 0.6471 - precision: 0.8877 - recall: 0.7116 - tn: 2674345.5000 - tp: 213662.8750 - val_accuracy: 0.8055 - val_auc: 0.9777 - val_fn: 2670.0000 - val_fp: 720.0000 - val_loss: 0.6077 - val_precision: 0.9106 - val_recall: 0.7330 - val_tn: 89280.0000 - val_tp: 7330.0000 - learning_rate: 0.0010\n",
      "Epoch 35/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.8004 - auc: 0.9755 - fn: 84851.0156 - fp: 26825.6973 - loss: 0.6356 - precision: 0.8890 - recall: 0.7167 - tn: 2674613.7500 - tp: 215308.9219 - val_accuracy: 0.8067 - val_auc: 0.9784 - val_fn: 2618.0000 - val_fp: 721.0000 - val_loss: 0.5991 - val_precision: 0.9110 - val_recall: 0.7382 - val_tn: 89279.0000 - val_tp: 7382.0000 - learning_rate: 0.0010\n",
      "Epoch 36/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 15ms/step - accuracy: 0.8042 - auc: 0.9761 - fn: 83158.3516 - fp: 26487.7695 - loss: 0.6264 - precision: 0.8909 - recall: 0.7225 - tn: 2674951.7500 - tp: 217001.5938 - val_accuracy: 0.8091 - val_auc: 0.9789 - val_fn: 2576.0000 - val_fp: 723.0000 - val_loss: 0.5905 - val_precision: 0.9113 - val_recall: 0.7424 - val_tn: 89277.0000 - val_tp: 7424.0000 - learning_rate: 0.0010\n",
      "Epoch 37/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 13ms/step - accuracy: 0.8075 - auc: 0.9770 - fn: 81639.0469 - fp: 26279.2559 - loss: 0.6143 - precision: 0.8925 - recall: 0.7279 - tn: 2675160.2500 - tp: 218520.9062 - val_accuracy: 0.8106 - val_auc: 0.9794 - val_fn: 2540.0000 - val_fp: 728.0000 - val_loss: 0.5825 - val_precision: 0.9111 - val_recall: 0.7460 - val_tn: 89272.0000 - val_tp: 7460.0000 - learning_rate: 0.0010\n",
      "Epoch 38/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 14ms/step - accuracy: 0.8105 - auc: 0.9775 - fn: 79983.2266 - fp: 26151.0566 - loss: 0.6058 - precision: 0.8941 - recall: 0.7334 - tn: 2675288.5000 - tp: 220176.7188 - val_accuracy: 0.8131 - val_auc: 0.9797 - val_fn: 2497.0000 - val_fp: 721.0000 - val_loss: 0.5760 - val_precision: 0.9123 - val_recall: 0.7503 - val_tn: 89279.0000 - val_tp: 7503.0000 - learning_rate: 0.0010\n",
      "Epoch 39/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 14ms/step - accuracy: 0.8132 - auc: 0.9781 - fn: 78831.9531 - fp: 25820.5957 - loss: 0.5978 - precision: 0.8954 - recall: 0.7373 - tn: 2675619.0000 - tp: 221328.0000 - val_accuracy: 0.8149 - val_auc: 0.9801 - val_fn: 2454.0000 - val_fp: 722.0000 - val_loss: 0.5689 - val_precision: 0.9127 - val_recall: 0.7546 - val_tn: 89278.0000 - val_tp: 7546.0000 - learning_rate: 0.0010\n",
      "Epoch 40/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 13ms/step - accuracy: 0.8156 - auc: 0.9786 - fn: 77706.9453 - fp: 25688.1543 - loss: 0.5897 - precision: 0.8965 - recall: 0.7410 - tn: 2675751.2500 - tp: 222453.0000 - val_accuracy: 0.8167 - val_auc: 0.9805 - val_fn: 2418.0000 - val_fp: 722.0000 - val_loss: 0.5626 - val_precision: 0.9131 - val_recall: 0.7582 - val_tn: 89278.0000 - val_tp: 7582.0000 - learning_rate: 0.0010\n",
      "Epoch 41/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 13ms/step - accuracy: 0.8186 - auc: 0.9793 - fn: 76326.2188 - fp: 25657.5059 - loss: 0.5794 - precision: 0.8968 - recall: 0.7451 - tn: 2675782.0000 - tp: 223833.7344 - val_accuracy: 0.8185 - val_auc: 0.9809 - val_fn: 2381.0000 - val_fp: 730.0000 - val_loss: 0.5561 - val_precision: 0.9126 - val_recall: 0.7619 - val_tn: 89270.0000 - val_tp: 7619.0000 - learning_rate: 0.0010\n",
      "Epoch 42/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 14ms/step - accuracy: 0.8216 - auc: 0.9797 - fn: 74949.4766 - fp: 25403.4551 - loss: 0.5725 - precision: 0.8985 - recall: 0.7499 - tn: 2676036.0000 - tp: 225210.4688 - val_accuracy: 0.8210 - val_auc: 0.9812 - val_fn: 2337.0000 - val_fp: 727.0000 - val_loss: 0.5496 - val_precision: 0.9133 - val_recall: 0.7663 - val_tn: 89273.0000 - val_tp: 7663.0000 - learning_rate: 0.0010\n",
      "Epoch 43/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 17ms/step - accuracy: 0.8234 - auc: 0.9803 - fn: 73811.5781 - fp: 25411.2676 - loss: 0.5634 - precision: 0.8988 - recall: 0.7542 - tn: 2676028.2500 - tp: 226348.3750 - val_accuracy: 0.8233 - val_auc: 0.9816 - val_fn: 2310.0000 - val_fp: 736.0000 - val_loss: 0.5430 - val_precision: 0.9127 - val_recall: 0.7690 - val_tn: 89264.0000 - val_tp: 7690.0000 - learning_rate: 0.0010\n",
      "Epoch 44/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 15ms/step - accuracy: 0.8262 - auc: 0.9805 - fn: 72714.1641 - fp: 25026.2793 - loss: 0.5579 - precision: 0.9008 - recall: 0.7575 - tn: 2676413.2500 - tp: 227445.7812 - val_accuracy: 0.8255 - val_auc: 0.9818 - val_fn: 2287.0000 - val_fp: 740.0000 - val_loss: 0.5384 - val_precision: 0.9125 - val_recall: 0.7713 - val_tn: 89260.0000 - val_tp: 7713.0000 - learning_rate: 0.0010\n",
      "Epoch 45/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 16ms/step - accuracy: 0.8280 - auc: 0.9811 - fn: 71823.1328 - fp: 24923.9453 - loss: 0.5500 - precision: 0.9014 - recall: 0.7605 - tn: 2676515.5000 - tp: 228336.8125 - val_accuracy: 0.8269 - val_auc: 0.9823 - val_fn: 2257.0000 - val_fp: 743.0000 - val_loss: 0.5309 - val_precision: 0.9124 - val_recall: 0.7743 - val_tn: 89257.0000 - val_tp: 7743.0000 - learning_rate: 0.0010\n",
      "Epoch 46/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 15ms/step - accuracy: 0.8309 - auc: 0.9816 - fn: 70802.6953 - fp: 24849.7734 - loss: 0.5431 - precision: 0.9025 - recall: 0.7640 - tn: 2676589.7500 - tp: 229357.2500 - val_accuracy: 0.8283 - val_auc: 0.9825 - val_fn: 2235.0000 - val_fp: 754.0000 - val_loss: 0.5261 - val_precision: 0.9115 - val_recall: 0.7765 - val_tn: 89246.0000 - val_tp: 7765.0000 - learning_rate: 0.0010\n",
      "Epoch 47/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 15ms/step - accuracy: 0.8319 - auc: 0.9819 - fn: 69824.8125 - fp: 24696.0039 - loss: 0.5382 - precision: 0.9028 - recall: 0.7668 - tn: 2676743.5000 - tp: 230335.1250 - val_accuracy: 0.8301 - val_auc: 0.9828 - val_fn: 2203.0000 - val_fp: 750.0000 - val_loss: 0.5197 - val_precision: 0.9122 - val_recall: 0.7797 - val_tn: 89250.0000 - val_tp: 7797.0000 - learning_rate: 0.0010\n",
      "Epoch 48/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 14ms/step - accuracy: 0.8345 - auc: 0.9824 - fn: 68803.5547 - fp: 24395.2168 - loss: 0.5304 - precision: 0.9048 - recall: 0.7708 - tn: 2677044.2500 - tp: 231356.3906 - val_accuracy: 0.8311 - val_auc: 0.9832 - val_fn: 2173.0000 - val_fp: 748.0000 - val_loss: 0.5144 - val_precision: 0.9128 - val_recall: 0.7827 - val_tn: 89252.0000 - val_tp: 7827.0000 - learning_rate: 0.0010\n",
      "Epoch 49/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 16ms/step - accuracy: 0.8367 - auc: 0.9826 - fn: 67834.9141 - fp: 24437.5801 - loss: 0.5240 - precision: 0.9048 - recall: 0.7738 - tn: 2677002.0000 - tp: 232325.0312 - val_accuracy: 0.8326 - val_auc: 0.9834 - val_fn: 2153.0000 - val_fp: 755.0000 - val_loss: 0.5108 - val_precision: 0.9122 - val_recall: 0.7847 - val_tn: 89245.0000 - val_tp: 7847.0000 - learning_rate: 0.0010\n",
      "Epoch 50/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 15ms/step - accuracy: 0.8383 - auc: 0.9828 - fn: 66929.9219 - fp: 24194.7793 - loss: 0.5203 - precision: 0.9059 - recall: 0.7770 - tn: 2677244.7500 - tp: 233230.0312 - val_accuracy: 0.8346 - val_auc: 0.9837 - val_fn: 2139.0000 - val_fp: 747.0000 - val_loss: 0.5058 - val_precision: 0.9132 - val_recall: 0.7861 - val_tn: 89253.0000 - val_tp: 7861.0000 - learning_rate: 0.0010\n",
      "Epoch 51/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.8401 - auc: 0.9831 - fn: 66162.9922 - fp: 24009.7441 - loss: 0.5149 - precision: 0.9070 - recall: 0.7792 - tn: 2677429.7500 - tp: 233996.9531 - val_accuracy: 0.8361 - val_auc: 0.9838 - val_fn: 2112.0000 - val_fp: 747.0000 - val_loss: 0.5016 - val_precision: 0.9135 - val_recall: 0.7888 - val_tn: 89253.0000 - val_tp: 7888.0000 - learning_rate: 0.0010\n",
      "Epoch 52/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 12ms/step - accuracy: 0.8421 - auc: 0.9835 - fn: 65372.2969 - fp: 24041.0176 - loss: 0.5095 - precision: 0.9069 - recall: 0.7818 - tn: 2677398.5000 - tp: 234787.6562 - val_accuracy: 0.8379 - val_auc: 0.9841 - val_fn: 2092.0000 - val_fp: 756.0000 - val_loss: 0.4963 - val_precision: 0.9127 - val_recall: 0.7908 - val_tn: 89244.0000 - val_tp: 7908.0000 - learning_rate: 0.0010\n",
      "Epoch 53/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 11ms/step - accuracy: 0.8430 - auc: 0.9836 - fn: 64542.6836 - fp: 23827.2461 - loss: 0.5047 - precision: 0.9082 - recall: 0.7848 - tn: 2677612.2500 - tp: 235617.2656 - val_accuracy: 0.8407 - val_auc: 0.9845 - val_fn: 2065.0000 - val_fp: 739.0000 - val_loss: 0.4902 - val_precision: 0.9148 - val_recall: 0.7935 - val_tn: 89261.0000 - val_tp: 7935.0000 - learning_rate: 0.0010\n",
      "Epoch 54/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 11ms/step - accuracy: 0.8455 - auc: 0.9839 - fn: 63962.4414 - fp: 23724.9668 - loss: 0.5006 - precision: 0.9086 - recall: 0.7866 - tn: 2677714.5000 - tp: 236197.5000 - val_accuracy: 0.8407 - val_auc: 0.9846 - val_fn: 2043.0000 - val_fp: 746.0000 - val_loss: 0.4883 - val_precision: 0.9143 - val_recall: 0.7957 - val_tn: 89254.0000 - val_tp: 7957.0000 - learning_rate: 0.0010\n",
      "Epoch 55/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - accuracy: 0.8467 - auc: 0.9843 - fn: 63314.5391 - fp: 23728.6953 - loss: 0.4941 - precision: 0.9095 - recall: 0.7892 - tn: 2677710.7500 - tp: 236845.4062 - val_accuracy: 0.8420 - val_auc: 0.9849 - val_fn: 2024.0000 - val_fp: 744.0000 - val_loss: 0.4832 - val_precision: 0.9147 - val_recall: 0.7976 - val_tn: 89256.0000 - val_tp: 7976.0000 - learning_rate: 0.0010\n",
      "Epoch 56/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - accuracy: 0.8476 - auc: 0.9845 - fn: 62628.7383 - fp: 23450.1387 - loss: 0.4908 - precision: 0.9102 - recall: 0.7910 - tn: 2677989.2500 - tp: 237531.2031 - val_accuracy: 0.8433 - val_auc: 0.9850 - val_fn: 2003.0000 - val_fp: 740.0000 - val_loss: 0.4792 - val_precision: 0.9153 - val_recall: 0.7997 - val_tn: 89260.0000 - val_tp: 7997.0000 - learning_rate: 0.0010\n",
      "Epoch 57/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 13ms/step - accuracy: 0.8492 - auc: 0.9846 - fn: 62015.4805 - fp: 23489.3945 - loss: 0.4871 - precision: 0.9103 - recall: 0.7933 - tn: 2677950.0000 - tp: 238144.4688 - val_accuracy: 0.8442 - val_auc: 0.9852 - val_fn: 1987.0000 - val_fp: 740.0000 - val_loss: 0.4755 - val_precision: 0.9155 - val_recall: 0.8013 - val_tn: 89260.0000 - val_tp: 8013.0000 - learning_rate: 0.0010\n",
      "Epoch 58/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 15ms/step - accuracy: 0.8502 - auc: 0.9849 - fn: 61438.3867 - fp: 23412.1504 - loss: 0.4821 - precision: 0.9108 - recall: 0.7948 - tn: 2678027.2500 - tp: 238721.5625 - val_accuracy: 0.8457 - val_auc: 0.9853 - val_fn: 1973.0000 - val_fp: 738.0000 - val_loss: 0.4714 - val_precision: 0.9158 - val_recall: 0.8027 - val_tn: 89262.0000 - val_tp: 8027.0000 - learning_rate: 0.0010\n",
      "Epoch 59/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.8512 - auc: 0.9851 - fn: 60770.9688 - fp: 23251.3594 - loss: 0.4787 - precision: 0.9115 - recall: 0.7973 - tn: 2678188.2500 - tp: 239388.9844 - val_accuracy: 0.8468 - val_auc: 0.9855 - val_fn: 1955.0000 - val_fp: 749.0000 - val_loss: 0.4684 - val_precision: 0.9148 - val_recall: 0.8045 - val_tn: 89251.0000 - val_tp: 8045.0000 - learning_rate: 0.0010\n",
      "Epoch 60/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 17ms/step - accuracy: 0.8528 - auc: 0.9853 - fn: 60318.8711 - fp: 23273.1074 - loss: 0.4763 - precision: 0.9113 - recall: 0.7986 - tn: 2678166.5000 - tp: 239841.0781 - val_accuracy: 0.8481 - val_auc: 0.9856 - val_fn: 1940.0000 - val_fp: 740.0000 - val_loss: 0.4639 - val_precision: 0.9159 - val_recall: 0.8060 - val_tn: 89260.0000 - val_tp: 8060.0000 - learning_rate: 0.0010\n",
      "Epoch 61/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 13ms/step - accuracy: 0.8538 - auc: 0.9854 - fn: 59660.1055 - fp: 23057.6191 - loss: 0.4719 - precision: 0.9125 - recall: 0.8011 - tn: 2678382.0000 - tp: 240499.8438 - val_accuracy: 0.8491 - val_auc: 0.9858 - val_fn: 1922.0000 - val_fp: 740.0000 - val_loss: 0.4617 - val_precision: 0.9161 - val_recall: 0.8078 - val_tn: 89260.0000 - val_tp: 8078.0000 - learning_rate: 0.0010\n",
      "Epoch 62/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.8558 - auc: 0.9858 - fn: 59055.5312 - fp: 22988.7461 - loss: 0.4664 - precision: 0.9132 - recall: 0.8033 - tn: 2678450.7500 - tp: 241104.4062 - val_accuracy: 0.8501 - val_auc: 0.9860 - val_fn: 1904.0000 - val_fp: 743.0000 - val_loss: 0.4570 - val_precision: 0.9159 - val_recall: 0.8096 - val_tn: 89257.0000 - val_tp: 8096.0000 - learning_rate: 0.0010\n",
      "Epoch 63/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 13ms/step - accuracy: 0.8567 - auc: 0.9858 - fn: 58520.4180 - fp: 22834.5039 - loss: 0.4638 - precision: 0.9136 - recall: 0.8050 - tn: 2678605.0000 - tp: 241639.5312 - val_accuracy: 0.8524 - val_auc: 0.9863 - val_fn: 1883.0000 - val_fp: 729.0000 - val_loss: 0.4525 - val_precision: 0.9176 - val_recall: 0.8117 - val_tn: 89271.0000 - val_tp: 8117.0000 - learning_rate: 0.0010\n",
      "Epoch 64/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 12ms/step - accuracy: 0.8573 - auc: 0.9860 - fn: 58015.5742 - fp: 22830.1914 - loss: 0.4609 - precision: 0.9139 - recall: 0.8068 - tn: 2678609.2500 - tp: 242144.3750 - val_accuracy: 0.8537 - val_auc: 0.9864 - val_fn: 1877.0000 - val_fp: 742.0000 - val_loss: 0.4498 - val_precision: 0.9163 - val_recall: 0.8123 - val_tn: 89258.0000 - val_tp: 8123.0000 - learning_rate: 0.0010\n",
      "Epoch 65/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 11ms/step - accuracy: 0.8585 - auc: 0.9862 - fn: 57583.3398 - fp: 22740.5781 - loss: 0.4571 - precision: 0.9143 - recall: 0.8081 - tn: 2678699.0000 - tp: 242576.6094 - val_accuracy: 0.8545 - val_auc: 0.9864 - val_fn: 1861.0000 - val_fp: 741.0000 - val_loss: 0.4475 - val_precision: 0.9166 - val_recall: 0.8139 - val_tn: 89259.0000 - val_tp: 8139.0000 - learning_rate: 0.0010\n",
      "Epoch 66/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.8592 - auc: 0.9863 - fn: 57193.4688 - fp: 22759.9238 - loss: 0.4544 - precision: 0.9144 - recall: 0.8094 - tn: 2678679.5000 - tp: 242966.4844 - val_accuracy: 0.8565 - val_auc: 0.9867 - val_fn: 1842.0000 - val_fp: 734.0000 - val_loss: 0.4437 - val_precision: 0.9175 - val_recall: 0.8158 - val_tn: 89266.0000 - val_tp: 8158.0000 - learning_rate: 0.0010\n",
      "Epoch 67/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 14ms/step - accuracy: 0.8600 - auc: 0.9865 - fn: 56764.3438 - fp: 22714.8535 - loss: 0.4509 - precision: 0.9147 - recall: 0.8108 - tn: 2678724.7500 - tp: 243395.6094 - val_accuracy: 0.8575 - val_auc: 0.9868 - val_fn: 1823.0000 - val_fp: 739.0000 - val_loss: 0.4414 - val_precision: 0.9171 - val_recall: 0.8177 - val_tn: 89261.0000 - val_tp: 8177.0000 - learning_rate: 0.0010\n",
      "Epoch 68/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 14ms/step - accuracy: 0.8617 - auc: 0.9868 - fn: 56041.2070 - fp: 22456.9336 - loss: 0.4463 - precision: 0.9159 - recall: 0.8135 - tn: 2678982.5000 - tp: 244118.7344 - val_accuracy: 0.8602 - val_auc: 0.9870 - val_fn: 1807.0000 - val_fp: 730.0000 - val_loss: 0.4381 - val_precision: 0.9182 - val_recall: 0.8193 - val_tn: 89270.0000 - val_tp: 8193.0000 - learning_rate: 0.0010\n",
      "Epoch 69/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 14ms/step - accuracy: 0.8632 - auc: 0.9868 - fn: 55597.1602 - fp: 22313.7129 - loss: 0.4438 - precision: 0.9166 - recall: 0.8148 - tn: 2679125.7500 - tp: 244562.7812 - val_accuracy: 0.8612 - val_auc: 0.9870 - val_fn: 1800.0000 - val_fp: 744.0000 - val_loss: 0.4360 - val_precision: 0.9168 - val_recall: 0.8200 - val_tn: 89256.0000 - val_tp: 8200.0000 - learning_rate: 0.0010\n",
      "Epoch 70/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 13ms/step - accuracy: 0.8634 - auc: 0.9869 - fn: 55372.5625 - fp: 22337.8379 - loss: 0.4422 - precision: 0.9165 - recall: 0.8154 - tn: 2679101.7500 - tp: 244787.3750 - val_accuracy: 0.8620 - val_auc: 0.9872 - val_fn: 1785.0000 - val_fp: 737.0000 - val_loss: 0.4333 - val_precision: 0.9177 - val_recall: 0.8215 - val_tn: 89263.0000 - val_tp: 8215.0000 - learning_rate: 0.0010\n",
      "Epoch 71/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 13ms/step - accuracy: 0.8637 - auc: 0.9869 - fn: 54994.9180 - fp: 22252.0957 - loss: 0.4408 - precision: 0.9165 - recall: 0.8162 - tn: 2679187.5000 - tp: 245165.0312 - val_accuracy: 0.8627 - val_auc: 0.9873 - val_fn: 1766.0000 - val_fp: 737.0000 - val_loss: 0.4315 - val_precision: 0.9178 - val_recall: 0.8234 - val_tn: 89263.0000 - val_tp: 8234.0000 - learning_rate: 0.0010\n",
      "Epoch 72/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 13ms/step - accuracy: 0.8652 - auc: 0.9871 - fn: 54500.6992 - fp: 22208.4688 - loss: 0.4369 - precision: 0.9167 - recall: 0.8181 - tn: 2679231.0000 - tp: 245659.2500 - val_accuracy: 0.8640 - val_auc: 0.9875 - val_fn: 1751.0000 - val_fp: 734.0000 - val_loss: 0.4282 - val_precision: 0.9183 - val_recall: 0.8249 - val_tn: 89266.0000 - val_tp: 8249.0000 - learning_rate: 0.0010\n",
      "Epoch 73/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 12ms/step - accuracy: 0.8658 - auc: 0.9873 - fn: 54129.0273 - fp: 22104.1035 - loss: 0.4333 - precision: 0.9176 - recall: 0.8197 - tn: 2679335.5000 - tp: 246030.9219 - val_accuracy: 0.8656 - val_auc: 0.9877 - val_fn: 1740.0000 - val_fp: 733.0000 - val_loss: 0.4257 - val_precision: 0.9185 - val_recall: 0.8260 - val_tn: 89267.0000 - val_tp: 8260.0000 - learning_rate: 0.0010\n",
      "Epoch 74/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 13ms/step - accuracy: 0.8675 - auc: 0.9875 - fn: 53786.7617 - fp: 22015.5762 - loss: 0.4307 - precision: 0.9180 - recall: 0.8209 - tn: 2679424.0000 - tp: 246373.1875 - val_accuracy: 0.8665 - val_auc: 0.9878 - val_fn: 1719.0000 - val_fp: 726.0000 - val_loss: 0.4221 - val_precision: 0.9194 - val_recall: 0.8281 - val_tn: 89274.0000 - val_tp: 8281.0000 - learning_rate: 0.0010\n",
      "Epoch 75/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.8679 - auc: 0.9876 - fn: 53226.8945 - fp: 21916.7188 - loss: 0.4282 - precision: 0.9182 - recall: 0.8223 - tn: 2679522.7500 - tp: 246933.0469 - val_accuracy: 0.8677 - val_auc: 0.9878 - val_fn: 1710.0000 - val_fp: 724.0000 - val_loss: 0.4204 - val_precision: 0.9197 - val_recall: 0.8290 - val_tn: 89276.0000 - val_tp: 8290.0000 - learning_rate: 0.0010\n",
      "Epoch 76/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 13ms/step - accuracy: 0.8688 - auc: 0.9878 - fn: 52740.9805 - fp: 21914.7031 - loss: 0.4236 - precision: 0.9188 - recall: 0.8246 - tn: 2679524.7500 - tp: 247418.9688 - val_accuracy: 0.8676 - val_auc: 0.9880 - val_fn: 1697.0000 - val_fp: 715.0000 - val_loss: 0.4185 - val_precision: 0.9207 - val_recall: 0.8303 - val_tn: 89285.0000 - val_tp: 8303.0000 - learning_rate: 0.0010\n",
      "Epoch 77/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 12ms/step - accuracy: 0.8698 - auc: 0.9879 - fn: 52606.9688 - fp: 21825.8301 - loss: 0.4228 - precision: 0.9193 - recall: 0.8249 - tn: 2679613.7500 - tp: 247552.9844 - val_accuracy: 0.8699 - val_auc: 0.9884 - val_fn: 1674.0000 - val_fp: 714.0000 - val_loss: 0.4134 - val_precision: 0.9210 - val_recall: 0.8326 - val_tn: 89286.0000 - val_tp: 8326.0000 - learning_rate: 0.0010\n",
      "Epoch 78/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 14ms/step - accuracy: 0.8700 - auc: 0.9880 - fn: 52394.1758 - fp: 21773.5215 - loss: 0.4206 - precision: 0.9192 - recall: 0.8257 - tn: 2679666.0000 - tp: 247765.7656 - val_accuracy: 0.8698 - val_auc: 0.9883 - val_fn: 1670.0000 - val_fp: 722.0000 - val_loss: 0.4128 - val_precision: 0.9202 - val_recall: 0.8330 - val_tn: 89278.0000 - val_tp: 8330.0000 - learning_rate: 0.0010\n",
      "Epoch 79/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 16ms/step - accuracy: 0.8703 - auc: 0.9880 - fn: 51973.3789 - fp: 21659.4355 - loss: 0.4179 - precision: 0.9194 - recall: 0.8267 - tn: 2679780.0000 - tp: 248186.5625 - val_accuracy: 0.8705 - val_auc: 0.9884 - val_fn: 1658.0000 - val_fp: 722.0000 - val_loss: 0.4110 - val_precision: 0.9203 - val_recall: 0.8342 - val_tn: 89278.0000 - val_tp: 8342.0000 - learning_rate: 0.0010\n",
      "Epoch 80/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 15ms/step - accuracy: 0.8720 - auc: 0.9881 - fn: 51689.9883 - fp: 21630.3477 - loss: 0.4161 - precision: 0.9201 - recall: 0.8278 - tn: 2679809.2500 - tp: 248469.9531 - val_accuracy: 0.8710 - val_auc: 0.9886 - val_fn: 1635.0000 - val_fp: 715.0000 - val_loss: 0.4078 - val_precision: 0.9213 - val_recall: 0.8365 - val_tn: 89285.0000 - val_tp: 8365.0000 - learning_rate: 0.0010\n",
      "Epoch 81/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 13ms/step - accuracy: 0.8719 - auc: 0.9883 - fn: 51239.8203 - fp: 21554.7598 - loss: 0.4136 - precision: 0.9204 - recall: 0.8294 - tn: 2679884.7500 - tp: 248920.1250 - val_accuracy: 0.8724 - val_auc: 0.9886 - val_fn: 1619.0000 - val_fp: 714.0000 - val_loss: 0.4063 - val_precision: 0.9215 - val_recall: 0.8381 - val_tn: 89286.0000 - val_tp: 8381.0000 - learning_rate: 0.0010\n",
      "Epoch 82/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 13ms/step - accuracy: 0.8727 - auc: 0.9883 - fn: 50962.6953 - fp: 21504.2305 - loss: 0.4117 - precision: 0.9209 - recall: 0.8301 - tn: 2679935.2500 - tp: 249197.2500 - val_accuracy: 0.8727 - val_auc: 0.9887 - val_fn: 1610.0000 - val_fp: 705.0000 - val_loss: 0.4043 - val_precision: 0.9225 - val_recall: 0.8390 - val_tn: 89295.0000 - val_tp: 8390.0000 - learning_rate: 0.0010\n",
      "Epoch 83/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 16ms/step - accuracy: 0.8740 - auc: 0.9886 - fn: 50586.0664 - fp: 21417.9160 - loss: 0.4074 - precision: 0.9211 - recall: 0.8317 - tn: 2680021.5000 - tp: 249573.8750 - val_accuracy: 0.8731 - val_auc: 0.9888 - val_fn: 1600.0000 - val_fp: 713.0000 - val_loss: 0.4017 - val_precision: 0.9218 - val_recall: 0.8400 - val_tn: 89287.0000 - val_tp: 8400.0000 - learning_rate: 0.0010\n",
      "Epoch 84/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 15ms/step - accuracy: 0.8737 - auc: 0.9885 - fn: 50375.3281 - fp: 21324.9004 - loss: 0.4078 - precision: 0.9212 - recall: 0.8319 - tn: 2680114.5000 - tp: 249784.6250 - val_accuracy: 0.8733 - val_auc: 0.9888 - val_fn: 1603.0000 - val_fp: 704.0000 - val_loss: 0.4013 - val_precision: 0.9226 - val_recall: 0.8397 - val_tn: 89296.0000 - val_tp: 8397.0000 - learning_rate: 0.0010\n",
      "Epoch 85/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 16ms/step - accuracy: 0.8747 - auc: 0.9887 - fn: 50059.5469 - fp: 21360.0176 - loss: 0.4044 - precision: 0.9213 - recall: 0.8334 - tn: 2680079.5000 - tp: 250100.3906 - val_accuracy: 0.8739 - val_auc: 0.9890 - val_fn: 1578.0000 - val_fp: 703.0000 - val_loss: 0.3979 - val_precision: 0.9230 - val_recall: 0.8422 - val_tn: 89297.0000 - val_tp: 8422.0000 - learning_rate: 0.0010\n",
      "Epoch 86/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 14ms/step - accuracy: 0.8769 - auc: 0.9888 - fn: 49501.5898 - fp: 21033.0410 - loss: 0.4019 - precision: 0.9229 - recall: 0.8354 - tn: 2680406.5000 - tp: 250658.3594 - val_accuracy: 0.8752 - val_auc: 0.9892 - val_fn: 1557.0000 - val_fp: 698.0000 - val_loss: 0.3957 - val_precision: 0.9236 - val_recall: 0.8443 - val_tn: 89302.0000 - val_tp: 8443.0000 - learning_rate: 0.0010\n",
      "Epoch 87/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.8764 - auc: 0.9889 - fn: 49634.5898 - fp: 21050.3438 - loss: 0.4004 - precision: 0.9226 - recall: 0.8347 - tn: 2680389.2500 - tp: 250525.3594 - val_accuracy: 0.8755 - val_auc: 0.9891 - val_fn: 1547.0000 - val_fp: 713.0000 - val_loss: 0.3953 - val_precision: 0.9222 - val_recall: 0.8453 - val_tn: 89287.0000 - val_tp: 8453.0000 - learning_rate: 0.0010\n",
      "Epoch 88/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 13ms/step - accuracy: 0.8774 - auc: 0.9889 - fn: 49269.5586 - fp: 21072.1680 - loss: 0.3993 - precision: 0.9229 - recall: 0.8361 - tn: 2680367.2500 - tp: 250890.3906 - val_accuracy: 0.8763 - val_auc: 0.9892 - val_fn: 1534.0000 - val_fp: 703.0000 - val_loss: 0.3935 - val_precision: 0.9233 - val_recall: 0.8466 - val_tn: 89297.0000 - val_tp: 8466.0000 - learning_rate: 0.0010\n",
      "Epoch 89/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 13ms/step - accuracy: 0.8777 - auc: 0.9891 - fn: 48948.5977 - fp: 21101.8516 - loss: 0.3966 - precision: 0.9225 - recall: 0.8370 - tn: 2680337.7500 - tp: 251211.3438 - val_accuracy: 0.8773 - val_auc: 0.9894 - val_fn: 1528.0000 - val_fp: 700.0000 - val_loss: 0.3894 - val_precision: 0.9237 - val_recall: 0.8472 - val_tn: 89300.0000 - val_tp: 8472.0000 - learning_rate: 0.0010\n",
      "Epoch 90/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 15ms/step - accuracy: 0.8778 - auc: 0.9891 - fn: 48690.0898 - fp: 20906.1875 - loss: 0.3956 - precision: 0.9232 - recall: 0.8375 - tn: 2680533.2500 - tp: 251469.8594 - val_accuracy: 0.8773 - val_auc: 0.9894 - val_fn: 1513.0000 - val_fp: 700.0000 - val_loss: 0.3896 - val_precision: 0.9238 - val_recall: 0.8487 - val_tn: 89300.0000 - val_tp: 8487.0000 - learning_rate: 0.0010\n",
      "Epoch 91/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 16ms/step - accuracy: 0.8785 - auc: 0.9891 - fn: 48244.6641 - fp: 20871.5820 - loss: 0.3943 - precision: 0.9236 - recall: 0.8391 - tn: 2680568.0000 - tp: 251915.2812 - val_accuracy: 0.8778 - val_auc: 0.9894 - val_fn: 1512.0000 - val_fp: 707.0000 - val_loss: 0.3881 - val_precision: 0.9231 - val_recall: 0.8488 - val_tn: 89293.0000 - val_tp: 8488.0000 - learning_rate: 0.0010\n",
      "Epoch 92/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 14ms/step - accuracy: 0.8797 - auc: 0.9893 - fn: 48075.4766 - fp: 20792.4551 - loss: 0.3916 - precision: 0.9237 - recall: 0.8397 - tn: 2680647.0000 - tp: 252084.4688 - val_accuracy: 0.8784 - val_auc: 0.9895 - val_fn: 1499.0000 - val_fp: 705.0000 - val_loss: 0.3867 - val_precision: 0.9234 - val_recall: 0.8501 - val_tn: 89295.0000 - val_tp: 8501.0000 - learning_rate: 0.0010\n",
      "Epoch 93/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.8805 - auc: 0.9894 - fn: 47723.8164 - fp: 20744.6914 - loss: 0.3894 - precision: 0.9242 - recall: 0.8411 - tn: 2680694.7500 - tp: 252436.1250 - val_accuracy: 0.8794 - val_auc: 0.9896 - val_fn: 1489.0000 - val_fp: 692.0000 - val_loss: 0.3840 - val_precision: 0.9248 - val_recall: 0.8511 - val_tn: 89308.0000 - val_tp: 8511.0000 - learning_rate: 0.0010\n",
      "Epoch 94/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 13ms/step - accuracy: 0.8804 - auc: 0.9894 - fn: 47557.3594 - fp: 20743.4766 - loss: 0.3878 - precision: 0.9240 - recall: 0.8416 - tn: 2680696.0000 - tp: 252602.5938 - val_accuracy: 0.8800 - val_auc: 0.9896 - val_fn: 1488.0000 - val_fp: 696.0000 - val_loss: 0.3831 - val_precision: 0.9244 - val_recall: 0.8512 - val_tn: 89304.0000 - val_tp: 8512.0000 - learning_rate: 0.0010\n",
      "Epoch 95/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 18ms/step - accuracy: 0.8816 - auc: 0.9896 - fn: 47249.5820 - fp: 20679.5449 - loss: 0.3848 - precision: 0.9246 - recall: 0.8428 - tn: 2680760.0000 - tp: 252910.3594 - val_accuracy: 0.8807 - val_auc: 0.9897 - val_fn: 1466.0000 - val_fp: 695.0000 - val_loss: 0.3801 - val_precision: 0.9247 - val_recall: 0.8534 - val_tn: 89305.0000 - val_tp: 8534.0000 - learning_rate: 0.0010\n",
      "Epoch 96/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 14ms/step - accuracy: 0.8819 - auc: 0.9897 - fn: 47107.3086 - fp: 20500.6465 - loss: 0.3834 - precision: 0.9253 - recall: 0.8431 - tn: 2680938.7500 - tp: 253052.6406 - val_accuracy: 0.8809 - val_auc: 0.9898 - val_fn: 1465.0000 - val_fp: 693.0000 - val_loss: 0.3798 - val_precision: 0.9249 - val_recall: 0.8535 - val_tn: 89307.0000 - val_tp: 8535.0000 - learning_rate: 0.0010\n",
      "Epoch 97/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 14ms/step - accuracy: 0.8822 - auc: 0.9897 - fn: 46878.5234 - fp: 20628.2227 - loss: 0.3819 - precision: 0.9251 - recall: 0.8441 - tn: 2680811.2500 - tp: 253281.4219 - val_accuracy: 0.8816 - val_auc: 0.9899 - val_fn: 1450.0000 - val_fp: 683.0000 - val_loss: 0.3771 - val_precision: 0.9260 - val_recall: 0.8550 - val_tn: 89317.0000 - val_tp: 8550.0000 - learning_rate: 0.0010\n",
      "Epoch 98/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.8821 - auc: 0.9897 - fn: 46838.9961 - fp: 20643.0156 - loss: 0.3822 - precision: 0.9247 - recall: 0.8438 - tn: 2680796.5000 - tp: 253320.9531 - val_accuracy: 0.8819 - val_auc: 0.9899 - val_fn: 1441.0000 - val_fp: 684.0000 - val_loss: 0.3752 - val_precision: 0.9260 - val_recall: 0.8559 - val_tn: 89316.0000 - val_tp: 8559.0000 - learning_rate: 0.0010\n",
      "Epoch 99/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.8828 - auc: 0.9898 - fn: 46508.2852 - fp: 20484.8086 - loss: 0.3800 - precision: 0.9252 - recall: 0.8449 - tn: 2680954.7500 - tp: 253651.6562 - val_accuracy: 0.8824 - val_auc: 0.9900 - val_fn: 1440.0000 - val_fp: 688.0000 - val_loss: 0.3746 - val_precision: 0.9256 - val_recall: 0.8560 - val_tn: 89312.0000 - val_tp: 8560.0000 - learning_rate: 0.0010\n",
      "Epoch 100/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 12ms/step - accuracy: 0.8835 - auc: 0.9898 - fn: 46260.6484 - fp: 20355.6562 - loss: 0.3788 - precision: 0.9256 - recall: 0.8456 - tn: 2681083.7500 - tp: 253899.2969 - val_accuracy: 0.8827 - val_auc: 0.9900 - val_fn: 1435.0000 - val_fp: 689.0000 - val_loss: 0.3734 - val_precision: 0.9255 - val_recall: 0.8565 - val_tn: 89311.0000 - val_tp: 8565.0000 - learning_rate: 0.0010\n",
      "Epoch 101/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 13ms/step - accuracy: 0.8843 - auc: 0.9899 - fn: 45987.9102 - fp: 20284.7793 - loss: 0.3760 - precision: 0.9261 - recall: 0.8467 - tn: 2681154.7500 - tp: 254172.0312 - val_accuracy: 0.8838 - val_auc: 0.9901 - val_fn: 1425.0000 - val_fp: 687.0000 - val_loss: 0.3719 - val_precision: 0.9258 - val_recall: 0.8575 - val_tn: 89313.0000 - val_tp: 8575.0000 - learning_rate: 0.0010\n",
      "Epoch 102/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 12ms/step - accuracy: 0.8844 - auc: 0.9900 - fn: 45997.1562 - fp: 20412.3105 - loss: 0.3751 - precision: 0.9258 - recall: 0.8472 - tn: 2681027.2500 - tp: 254162.7812 - val_accuracy: 0.8844 - val_auc: 0.9901 - val_fn: 1422.0000 - val_fp: 692.0000 - val_loss: 0.3714 - val_precision: 0.9254 - val_recall: 0.8578 - val_tn: 89308.0000 - val_tp: 8578.0000 - learning_rate: 0.0010\n",
      "Epoch 103/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 12ms/step - accuracy: 0.8847 - auc: 0.9900 - fn: 45622.8906 - fp: 20369.0273 - loss: 0.3739 - precision: 0.9261 - recall: 0.8482 - tn: 2681070.5000 - tp: 254537.0625 - val_accuracy: 0.8852 - val_auc: 0.9902 - val_fn: 1416.0000 - val_fp: 679.0000 - val_loss: 0.3685 - val_precision: 0.9267 - val_recall: 0.8584 - val_tn: 89321.0000 - val_tp: 8584.0000 - learning_rate: 0.0010\n",
      "Epoch 104/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 11ms/step - accuracy: 0.8857 - auc: 0.9902 - fn: 45329.9414 - fp: 20079.8594 - loss: 0.3714 - precision: 0.9271 - recall: 0.8492 - tn: 2681359.7500 - tp: 254830.0000 - val_accuracy: 0.8853 - val_auc: 0.9902 - val_fn: 1403.0000 - val_fp: 682.0000 - val_loss: 0.3668 - val_precision: 0.9265 - val_recall: 0.8597 - val_tn: 89318.0000 - val_tp: 8597.0000 - learning_rate: 0.0010\n",
      "Epoch 105/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 11ms/step - accuracy: 0.8856 - auc: 0.9901 - fn: 45335.5586 - fp: 20141.3398 - loss: 0.3711 - precision: 0.9267 - recall: 0.8490 - tn: 2681298.2500 - tp: 254824.3906 - val_accuracy: 0.8855 - val_auc: 0.9903 - val_fn: 1401.0000 - val_fp: 686.0000 - val_loss: 0.3658 - val_precision: 0.9261 - val_recall: 0.8599 - val_tn: 89314.0000 - val_tp: 8599.0000 - learning_rate: 0.0010\n",
      "Epoch 106/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 10ms/step - accuracy: 0.8863 - auc: 0.9901 - fn: 45203.6836 - fp: 20122.3691 - loss: 0.3706 - precision: 0.9269 - recall: 0.8495 - tn: 2681317.2500 - tp: 254956.2656 - val_accuracy: 0.8857 - val_auc: 0.9903 - val_fn: 1393.0000 - val_fp: 684.0000 - val_loss: 0.3647 - val_precision: 0.9264 - val_recall: 0.8607 - val_tn: 89316.0000 - val_tp: 8607.0000 - learning_rate: 0.0010\n",
      "Epoch 107/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 10ms/step - accuracy: 0.8865 - auc: 0.9903 - fn: 44959.1445 - fp: 20071.0293 - loss: 0.3681 - precision: 0.9272 - recall: 0.8502 - tn: 2681368.5000 - tp: 255200.7969 - val_accuracy: 0.8860 - val_auc: 0.9903 - val_fn: 1387.0000 - val_fp: 691.0000 - val_loss: 0.3641 - val_precision: 0.9257 - val_recall: 0.8613 - val_tn: 89309.0000 - val_tp: 8613.0000 - learning_rate: 0.0010\n",
      "Epoch 108/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 10ms/step - accuracy: 0.8877 - auc: 0.9904 - fn: 44654.6641 - fp: 19968.3711 - loss: 0.3659 - precision: 0.9278 - recall: 0.8515 - tn: 2681471.2500 - tp: 255505.2812 - val_accuracy: 0.8872 - val_auc: 0.9904 - val_fn: 1382.0000 - val_fp: 674.0000 - val_loss: 0.3620 - val_precision: 0.9275 - val_recall: 0.8618 - val_tn: 89326.0000 - val_tp: 8618.0000 - learning_rate: 0.0010\n",
      "Epoch 109/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 11ms/step - accuracy: 0.8870 - auc: 0.9903 - fn: 44557.2695 - fp: 20007.7227 - loss: 0.3664 - precision: 0.9273 - recall: 0.8516 - tn: 2681431.7500 - tp: 255602.6719 - val_accuracy: 0.8877 - val_auc: 0.9905 - val_fn: 1374.0000 - val_fp: 676.0000 - val_loss: 0.3603 - val_precision: 0.9273 - val_recall: 0.8626 - val_tn: 89324.0000 - val_tp: 8626.0000 - learning_rate: 0.0010\n",
      "Epoch 110/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 12ms/step - accuracy: 0.8876 - auc: 0.9905 - fn: 44397.3086 - fp: 19958.8027 - loss: 0.3640 - precision: 0.9277 - recall: 0.8522 - tn: 2681480.7500 - tp: 255762.6406 - val_accuracy: 0.8878 - val_auc: 0.9905 - val_fn: 1367.0000 - val_fp: 680.0000 - val_loss: 0.3600 - val_precision: 0.9270 - val_recall: 0.8633 - val_tn: 89320.0000 - val_tp: 8633.0000 - learning_rate: 0.0010\n",
      "Epoch 111/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 12ms/step - accuracy: 0.8884 - auc: 0.9906 - fn: 44108.1016 - fp: 19762.2461 - loss: 0.3624 - precision: 0.9285 - recall: 0.8532 - tn: 2681677.2500 - tp: 256051.8438 - val_accuracy: 0.8878 - val_auc: 0.9906 - val_fn: 1354.0000 - val_fp: 670.0000 - val_loss: 0.3575 - val_precision: 0.9281 - val_recall: 0.8646 - val_tn: 89330.0000 - val_tp: 8646.0000 - learning_rate: 0.0010\n",
      "Epoch 112/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 13ms/step - accuracy: 0.8886 - auc: 0.9905 - fn: 43963.5938 - fp: 19742.9336 - loss: 0.3625 - precision: 0.9285 - recall: 0.8533 - tn: 2681696.5000 - tp: 256196.3438 - val_accuracy: 0.8885 - val_auc: 0.9906 - val_fn: 1351.0000 - val_fp: 680.0000 - val_loss: 0.3572 - val_precision: 0.9271 - val_recall: 0.8649 - val_tn: 89320.0000 - val_tp: 8649.0000 - learning_rate: 0.0010\n",
      "Epoch 113/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 13ms/step - accuracy: 0.8888 - auc: 0.9906 - fn: 43842.2930 - fp: 19767.6230 - loss: 0.3597 - precision: 0.9283 - recall: 0.8539 - tn: 2681672.0000 - tp: 256317.6562 - val_accuracy: 0.8897 - val_auc: 0.9906 - val_fn: 1349.0000 - val_fp: 668.0000 - val_loss: 0.3550 - val_precision: 0.9283 - val_recall: 0.8651 - val_tn: 89332.0000 - val_tp: 8651.0000 - learning_rate: 0.0010\n",
      "Epoch 114/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 15ms/step - accuracy: 0.8894 - auc: 0.9906 - fn: 43725.8711 - fp: 19710.2441 - loss: 0.3597 - precision: 0.9286 - recall: 0.8542 - tn: 2681729.2500 - tp: 256434.0781 - val_accuracy: 0.8896 - val_auc: 0.9907 - val_fn: 1332.0000 - val_fp: 670.0000 - val_loss: 0.3538 - val_precision: 0.9283 - val_recall: 0.8668 - val_tn: 89330.0000 - val_tp: 8668.0000 - learning_rate: 0.0010\n",
      "Epoch 115/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 17ms/step - accuracy: 0.8899 - auc: 0.9907 - fn: 43400.2109 - fp: 19517.6855 - loss: 0.3575 - precision: 0.9295 - recall: 0.8554 - tn: 2681921.7500 - tp: 256759.7344 - val_accuracy: 0.8900 - val_auc: 0.9908 - val_fn: 1330.0000 - val_fp: 672.0000 - val_loss: 0.3527 - val_precision: 0.9281 - val_recall: 0.8670 - val_tn: 89328.0000 - val_tp: 8670.0000 - learning_rate: 0.0010\n",
      "Epoch 116/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 16ms/step - accuracy: 0.8904 - auc: 0.9908 - fn: 43329.1484 - fp: 19752.5098 - loss: 0.3559 - precision: 0.9286 - recall: 0.8556 - tn: 2681687.0000 - tp: 256830.7969 - val_accuracy: 0.8903 - val_auc: 0.9907 - val_fn: 1314.0000 - val_fp: 673.0000 - val_loss: 0.3522 - val_precision: 0.9281 - val_recall: 0.8686 - val_tn: 89327.0000 - val_tp: 8686.0000 - learning_rate: 0.0010\n",
      "Epoch 117/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 16ms/step - accuracy: 0.8907 - auc: 0.9908 - fn: 42944.7031 - fp: 19501.2461 - loss: 0.3552 - precision: 0.9294 - recall: 0.8571 - tn: 2681938.2500 - tp: 257215.2344 - val_accuracy: 0.8905 - val_auc: 0.9908 - val_fn: 1312.0000 - val_fp: 674.0000 - val_loss: 0.3512 - val_precision: 0.9280 - val_recall: 0.8688 - val_tn: 89326.0000 - val_tp: 8688.0000 - learning_rate: 0.0010\n",
      "Epoch 118/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 17ms/step - accuracy: 0.8909 - auc: 0.9909 - fn: 42904.7461 - fp: 19584.7051 - loss: 0.3531 - precision: 0.9292 - recall: 0.8573 - tn: 2681854.7500 - tp: 257255.2031 - val_accuracy: 0.8909 - val_auc: 0.9908 - val_fn: 1310.0000 - val_fp: 671.0000 - val_loss: 0.3499 - val_precision: 0.9283 - val_recall: 0.8690 - val_tn: 89329.0000 - val_tp: 8690.0000 - learning_rate: 0.0010\n",
      "Epoch 119/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 15ms/step - accuracy: 0.8909 - auc: 0.9909 - fn: 42987.2266 - fp: 19514.3730 - loss: 0.3532 - precision: 0.9295 - recall: 0.8565 - tn: 2681925.2500 - tp: 257172.7188 - val_accuracy: 0.8910 - val_auc: 0.9908 - val_fn: 1301.0000 - val_fp: 673.0000 - val_loss: 0.3494 - val_precision: 0.9282 - val_recall: 0.8699 - val_tn: 89327.0000 - val_tp: 8699.0000 - learning_rate: 0.0010\n",
      "Epoch 120/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 12ms/step - accuracy: 0.8920 - auc: 0.9910 - fn: 42688.9023 - fp: 19473.7383 - loss: 0.3518 - precision: 0.9298 - recall: 0.8579 - tn: 2681965.7500 - tp: 257471.0469 - val_accuracy: 0.8918 - val_auc: 0.9908 - val_fn: 1292.0000 - val_fp: 674.0000 - val_loss: 0.3485 - val_precision: 0.9282 - val_recall: 0.8708 - val_tn: 89326.0000 - val_tp: 8708.0000 - learning_rate: 0.0010\n",
      "Epoch 121/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 12ms/step - accuracy: 0.8920 - auc: 0.9909 - fn: 42468.0234 - fp: 19462.6699 - loss: 0.3509 - precision: 0.9299 - recall: 0.8587 - tn: 2681976.7500 - tp: 257691.9219 - val_accuracy: 0.8922 - val_auc: 0.9909 - val_fn: 1283.0000 - val_fp: 680.0000 - val_loss: 0.3464 - val_precision: 0.9276 - val_recall: 0.8717 - val_tn: 89320.0000 - val_tp: 8717.0000 - learning_rate: 0.0010\n",
      "Epoch 122/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 14ms/step - accuracy: 0.8929 - auc: 0.9911 - fn: 42271.3633 - fp: 19299.5469 - loss: 0.3488 - precision: 0.9305 - recall: 0.8592 - tn: 2682140.0000 - tp: 257888.5781 - val_accuracy: 0.8930 - val_auc: 0.9909 - val_fn: 1280.0000 - val_fp: 679.0000 - val_loss: 0.3456 - val_precision: 0.9278 - val_recall: 0.8720 - val_tn: 89321.0000 - val_tp: 8720.0000 - learning_rate: 0.0010\n",
      "Epoch 123/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 13ms/step - accuracy: 0.8926 - auc: 0.9911 - fn: 42121.3477 - fp: 19377.6133 - loss: 0.3487 - precision: 0.9302 - recall: 0.8596 - tn: 2682062.0000 - tp: 258038.5938 - val_accuracy: 0.8928 - val_auc: 0.9909 - val_fn: 1281.0000 - val_fp: 671.0000 - val_loss: 0.3456 - val_precision: 0.9285 - val_recall: 0.8719 - val_tn: 89329.0000 - val_tp: 8719.0000 - learning_rate: 0.0010\n",
      "Epoch 124/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 13ms/step - accuracy: 0.8930 - auc: 0.9913 - fn: 41814.1172 - fp: 19300.7520 - loss: 0.3458 - precision: 0.9306 - recall: 0.8608 - tn: 2682138.7500 - tp: 258345.8281 - val_accuracy: 0.8933 - val_auc: 0.9910 - val_fn: 1266.0000 - val_fp: 665.0000 - val_loss: 0.3433 - val_precision: 0.9292 - val_recall: 0.8734 - val_tn: 89335.0000 - val_tp: 8734.0000 - learning_rate: 0.0010\n",
      "Epoch 125/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 15ms/step - accuracy: 0.8940 - auc: 0.9912 - fn: 41784.1289 - fp: 19115.4023 - loss: 0.3450 - precision: 0.9316 - recall: 0.8610 - tn: 2682324.0000 - tp: 258375.8125 - val_accuracy: 0.8936 - val_auc: 0.9911 - val_fn: 1273.0000 - val_fp: 672.0000 - val_loss: 0.3427 - val_precision: 0.9285 - val_recall: 0.8727 - val_tn: 89328.0000 - val_tp: 8727.0000 - learning_rate: 0.0010\n",
      "Epoch 126/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 13ms/step - accuracy: 0.8938 - auc: 0.9912 - fn: 41754.3711 - fp: 19180.5781 - loss: 0.3457 - precision: 0.9309 - recall: 0.8607 - tn: 2682259.0000 - tp: 258405.5781 - val_accuracy: 0.8948 - val_auc: 0.9911 - val_fn: 1266.0000 - val_fp: 669.0000 - val_loss: 0.3412 - val_precision: 0.9289 - val_recall: 0.8734 - val_tn: 89331.0000 - val_tp: 8734.0000 - learning_rate: 0.0010\n",
      "Epoch 127/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - accuracy: 0.8937 - auc: 0.9914 - fn: 41634.1172 - fp: 19047.6289 - loss: 0.3432 - precision: 0.9314 - recall: 0.8611 - tn: 2682392.0000 - tp: 258525.8281 - val_accuracy: 0.8944 - val_auc: 0.9910 - val_fn: 1261.0000 - val_fp: 674.0000 - val_loss: 0.3415 - val_precision: 0.9284 - val_recall: 0.8739 - val_tn: 89326.0000 - val_tp: 8739.0000 - learning_rate: 0.0010\n",
      "Epoch 128/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 14ms/step - accuracy: 0.8944 - auc: 0.9914 - fn: 41461.2148 - fp: 19094.5488 - loss: 0.3419 - precision: 0.9313 - recall: 0.8618 - tn: 2682345.0000 - tp: 258698.7344 - val_accuracy: 0.8952 - val_auc: 0.9912 - val_fn: 1252.0000 - val_fp: 668.0000 - val_loss: 0.3391 - val_precision: 0.9291 - val_recall: 0.8748 - val_tn: 89332.0000 - val_tp: 8748.0000 - learning_rate: 0.0010\n",
      "Epoch 129/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 13ms/step - accuracy: 0.8945 - auc: 0.9914 - fn: 41237.6953 - fp: 19095.3809 - loss: 0.3418 - precision: 0.9313 - recall: 0.8623 - tn: 2682344.2500 - tp: 258922.2500 - val_accuracy: 0.8960 - val_auc: 0.9913 - val_fn: 1251.0000 - val_fp: 670.0000 - val_loss: 0.3386 - val_precision: 0.9289 - val_recall: 0.8749 - val_tn: 89330.0000 - val_tp: 8749.0000 - learning_rate: 0.0010\n",
      "Epoch 130/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 14ms/step - accuracy: 0.8944 - auc: 0.9913 - fn: 41380.7148 - fp: 19073.6934 - loss: 0.3426 - precision: 0.9311 - recall: 0.8617 - tn: 2682365.7500 - tp: 258779.2344 - val_accuracy: 0.8953 - val_auc: 0.9912 - val_fn: 1252.0000 - val_fp: 668.0000 - val_loss: 0.3382 - val_precision: 0.9291 - val_recall: 0.8748 - val_tn: 89332.0000 - val_tp: 8748.0000 - learning_rate: 0.0010\n",
      "Epoch 131/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 16ms/step - accuracy: 0.8957 - auc: 0.9915 - fn: 41049.6445 - fp: 18975.8418 - loss: 0.3389 - precision: 0.9319 - recall: 0.8633 - tn: 2682463.7500 - tp: 259110.2969 - val_accuracy: 0.8957 - val_auc: 0.9913 - val_fn: 1244.0000 - val_fp: 668.0000 - val_loss: 0.3374 - val_precision: 0.9291 - val_recall: 0.8756 - val_tn: 89332.0000 - val_tp: 8756.0000 - learning_rate: 0.0010\n",
      "Epoch 132/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 15ms/step - accuracy: 0.8963 - auc: 0.9916 - fn: 40762.9336 - fp: 18858.7422 - loss: 0.3375 - precision: 0.9323 - recall: 0.8643 - tn: 2682580.7500 - tp: 259397.0156 - val_accuracy: 0.8970 - val_auc: 0.9913 - val_fn: 1232.0000 - val_fp: 665.0000 - val_loss: 0.3353 - val_precision: 0.9295 - val_recall: 0.8768 - val_tn: 89335.0000 - val_tp: 8768.0000 - learning_rate: 0.0010\n",
      "Epoch 133/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 16ms/step - accuracy: 0.8959 - auc: 0.9916 - fn: 40778.1445 - fp: 18811.9180 - loss: 0.3376 - precision: 0.9326 - recall: 0.8643 - tn: 2682627.5000 - tp: 259381.7969 - val_accuracy: 0.8967 - val_auc: 0.9913 - val_fn: 1238.0000 - val_fp: 663.0000 - val_loss: 0.3354 - val_precision: 0.9297 - val_recall: 0.8762 - val_tn: 89337.0000 - val_tp: 8762.0000 - learning_rate: 0.0010\n",
      "Epoch 134/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 15ms/step - accuracy: 0.8962 - auc: 0.9916 - fn: 40677.9727 - fp: 18941.4297 - loss: 0.3371 - precision: 0.9317 - recall: 0.8642 - tn: 2682498.0000 - tp: 259481.9688 - val_accuracy: 0.8972 - val_auc: 0.9913 - val_fn: 1229.0000 - val_fp: 663.0000 - val_loss: 0.3352 - val_precision: 0.9297 - val_recall: 0.8771 - val_tn: 89337.0000 - val_tp: 8771.0000 - learning_rate: 0.0010\n",
      "Epoch 135/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 15ms/step - accuracy: 0.8964 - auc: 0.9916 - fn: 40528.2539 - fp: 18959.7500 - loss: 0.3359 - precision: 0.9317 - recall: 0.8650 - tn: 2682479.7500 - tp: 259631.6875 - val_accuracy: 0.8981 - val_auc: 0.9914 - val_fn: 1223.0000 - val_fp: 661.0000 - val_loss: 0.3323 - val_precision: 0.9300 - val_recall: 0.8777 - val_tn: 89339.0000 - val_tp: 8777.0000 - learning_rate: 0.0010\n",
      "Epoch 136/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 15ms/step - accuracy: 0.8973 - auc: 0.9918 - fn: 40289.4805 - fp: 18681.8086 - loss: 0.3333 - precision: 0.9331 - recall: 0.8660 - tn: 2682757.7500 - tp: 259870.4688 - val_accuracy: 0.8981 - val_auc: 0.9914 - val_fn: 1217.0000 - val_fp: 661.0000 - val_loss: 0.3324 - val_precision: 0.9300 - val_recall: 0.8783 - val_tn: 89339.0000 - val_tp: 8783.0000 - learning_rate: 0.0010\n",
      "Epoch 137/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 16ms/step - accuracy: 0.8973 - auc: 0.9916 - fn: 40223.5938 - fp: 18751.0449 - loss: 0.3346 - precision: 0.9328 - recall: 0.8661 - tn: 2682688.5000 - tp: 259936.3438 - val_accuracy: 0.8982 - val_auc: 0.9913 - val_fn: 1216.0000 - val_fp: 654.0000 - val_loss: 0.3325 - val_precision: 0.9307 - val_recall: 0.8784 - val_tn: 89346.0000 - val_tp: 8784.0000 - learning_rate: 0.0010\n",
      "Epoch 138/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 16ms/step - accuracy: 0.8978 - auc: 0.9918 - fn: 39991.2891 - fp: 18692.4082 - loss: 0.3320 - precision: 0.9328 - recall: 0.8667 - tn: 2682747.0000 - tp: 260168.6562 - val_accuracy: 0.8992 - val_auc: 0.9914 - val_fn: 1206.0000 - val_fp: 652.0000 - val_loss: 0.3301 - val_precision: 0.9310 - val_recall: 0.8794 - val_tn: 89348.0000 - val_tp: 8794.0000 - learning_rate: 0.0010\n",
      "Epoch 139/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 15ms/step - accuracy: 0.8981 - auc: 0.9917 - fn: 39961.4727 - fp: 18615.2383 - loss: 0.3320 - precision: 0.9333 - recall: 0.8669 - tn: 2682824.2500 - tp: 260198.4688 - val_accuracy: 0.8981 - val_auc: 0.9914 - val_fn: 1206.0000 - val_fp: 654.0000 - val_loss: 0.3307 - val_precision: 0.9308 - val_recall: 0.8794 - val_tn: 89346.0000 - val_tp: 8794.0000 - learning_rate: 0.0010\n",
      "Epoch 140/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 16ms/step - accuracy: 0.8981 - auc: 0.9918 - fn: 39871.5156 - fp: 18621.6035 - loss: 0.3310 - precision: 0.9332 - recall: 0.8670 - tn: 2682818.0000 - tp: 260288.4219 - val_accuracy: 0.8987 - val_auc: 0.9915 - val_fn: 1200.0000 - val_fp: 656.0000 - val_loss: 0.3304 - val_precision: 0.9306 - val_recall: 0.8800 - val_tn: 89344.0000 - val_tp: 8800.0000 - learning_rate: 0.0010\n",
      "Epoch 141/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 19ms/step - accuracy: 0.8990 - auc: 0.9919 - fn: 39546.5820 - fp: 18641.5020 - loss: 0.3289 - precision: 0.9333 - recall: 0.8683 - tn: 2682798.0000 - tp: 260613.3594 - val_accuracy: 0.8989 - val_auc: 0.9915 - val_fn: 1197.0000 - val_fp: 651.0000 - val_loss: 0.3285 - val_precision: 0.9311 - val_recall: 0.8803 - val_tn: 89349.0000 - val_tp: 8803.0000 - learning_rate: 0.0010\n",
      "Epoch 142/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 19ms/step - accuracy: 0.8991 - auc: 0.9920 - fn: 39437.2188 - fp: 18508.2402 - loss: 0.3287 - precision: 0.9336 - recall: 0.8686 - tn: 2682931.2500 - tp: 260722.7344 - val_accuracy: 0.9000 - val_auc: 0.9915 - val_fn: 1193.0000 - val_fp: 651.0000 - val_loss: 0.3278 - val_precision: 0.9312 - val_recall: 0.8807 - val_tn: 89349.0000 - val_tp: 8807.0000 - learning_rate: 0.0010\n",
      "Epoch 143/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 19ms/step - accuracy: 0.8986 - auc: 0.9919 - fn: 39572.4609 - fp: 18554.5781 - loss: 0.3294 - precision: 0.9335 - recall: 0.8679 - tn: 2682885.0000 - tp: 260587.4844 - val_accuracy: 0.8995 - val_auc: 0.9915 - val_fn: 1185.0000 - val_fp: 647.0000 - val_loss: 0.3280 - val_precision: 0.9316 - val_recall: 0.8815 - val_tn: 89353.0000 - val_tp: 8815.0000 - learning_rate: 0.0010\n",
      "Epoch 144/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 18ms/step - accuracy: 0.9002 - auc: 0.9921 - fn: 39146.1289 - fp: 18398.6758 - loss: 0.3240 - precision: 0.9342 - recall: 0.8700 - tn: 2683040.7500 - tp: 261013.8125 - val_accuracy: 0.8999 - val_auc: 0.9916 - val_fn: 1181.0000 - val_fp: 644.0000 - val_loss: 0.3271 - val_precision: 0.9319 - val_recall: 0.8819 - val_tn: 89356.0000 - val_tp: 8819.0000 - learning_rate: 0.0010\n",
      "Epoch 145/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 18ms/step - accuracy: 0.8998 - auc: 0.9920 - fn: 39124.8203 - fp: 18398.9922 - loss: 0.3267 - precision: 0.9340 - recall: 0.8696 - tn: 2683040.5000 - tp: 261035.1250 - val_accuracy: 0.9008 - val_auc: 0.9916 - val_fn: 1178.0000 - val_fp: 642.0000 - val_loss: 0.3246 - val_precision: 0.9322 - val_recall: 0.8822 - val_tn: 89358.0000 - val_tp: 8822.0000 - learning_rate: 0.0010\n",
      "Epoch 146/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 20ms/step - accuracy: 0.9000 - auc: 0.9921 - fn: 39278.0859 - fp: 18405.8184 - loss: 0.3261 - precision: 0.9341 - recall: 0.8691 - tn: 2683033.7500 - tp: 260881.8594 - val_accuracy: 0.9010 - val_auc: 0.9916 - val_fn: 1176.0000 - val_fp: 644.0000 - val_loss: 0.3250 - val_precision: 0.9320 - val_recall: 0.8824 - val_tn: 89356.0000 - val_tp: 8824.0000 - learning_rate: 0.0010\n",
      "Epoch 147/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 18ms/step - accuracy: 0.8998 - auc: 0.9921 - fn: 39041.9102 - fp: 18348.8457 - loss: 0.3240 - precision: 0.9345 - recall: 0.8702 - tn: 2683090.7500 - tp: 261118.0312 - val_accuracy: 0.9009 - val_auc: 0.9916 - val_fn: 1175.0000 - val_fp: 646.0000 - val_loss: 0.3242 - val_precision: 0.9318 - val_recall: 0.8825 - val_tn: 89354.0000 - val_tp: 8825.0000 - learning_rate: 0.0010\n",
      "Epoch 148/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 18ms/step - accuracy: 0.9004 - auc: 0.9922 - fn: 38924.5586 - fp: 18361.5137 - loss: 0.3231 - precision: 0.9344 - recall: 0.8700 - tn: 2683078.0000 - tp: 261235.3906 - val_accuracy: 0.9007 - val_auc: 0.9916 - val_fn: 1170.0000 - val_fp: 640.0000 - val_loss: 0.3238 - val_precision: 0.9324 - val_recall: 0.8830 - val_tn: 89360.0000 - val_tp: 8830.0000 - learning_rate: 0.0010\n",
      "Epoch 149/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 18ms/step - accuracy: 0.9008 - auc: 0.9922 - fn: 38775.0508 - fp: 18275.7656 - loss: 0.3220 - precision: 0.9346 - recall: 0.8709 - tn: 2683163.7500 - tp: 261384.8906 - val_accuracy: 0.9021 - val_auc: 0.9917 - val_fn: 1163.0000 - val_fp: 642.0000 - val_loss: 0.3234 - val_precision: 0.9323 - val_recall: 0.8837 - val_tn: 89358.0000 - val_tp: 8837.0000 - learning_rate: 0.0010\n",
      "Epoch 150/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 15ms/step - accuracy: 0.9010 - auc: 0.9920 - fn: 38723.5195 - fp: 18256.4551 - loss: 0.3222 - precision: 0.9348 - recall: 0.8711 - tn: 2683183.0000 - tp: 261436.4219 - val_accuracy: 0.9023 - val_auc: 0.9917 - val_fn: 1161.0000 - val_fp: 641.0000 - val_loss: 0.3219 - val_precision: 0.9324 - val_recall: 0.8839 - val_tn: 89359.0000 - val_tp: 8839.0000 - learning_rate: 0.0010\n",
      "Epoch 151/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 16ms/step - accuracy: 0.9008 - auc: 0.9922 - fn: 38673.4141 - fp: 18262.1035 - loss: 0.3213 - precision: 0.9348 - recall: 0.8711 - tn: 2683177.5000 - tp: 261486.5312 - val_accuracy: 0.9025 - val_auc: 0.9917 - val_fn: 1158.0000 - val_fp: 632.0000 - val_loss: 0.3205 - val_precision: 0.9333 - val_recall: 0.8842 - val_tn: 89368.0000 - val_tp: 8842.0000 - learning_rate: 0.0010\n",
      "Epoch 152/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 20ms/step - accuracy: 0.9008 - auc: 0.9922 - fn: 38648.9180 - fp: 18360.0430 - loss: 0.3220 - precision: 0.9346 - recall: 0.8712 - tn: 2683079.5000 - tp: 261511.0312 - val_accuracy: 0.9024 - val_auc: 0.9918 - val_fn: 1155.0000 - val_fp: 635.0000 - val_loss: 0.3197 - val_precision: 0.9330 - val_recall: 0.8845 - val_tn: 89365.0000 - val_tp: 8845.0000 - learning_rate: 0.0010\n",
      "Epoch 153/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 17ms/step - accuracy: 0.9014 - auc: 0.9922 - fn: 38420.6133 - fp: 18124.7285 - loss: 0.3207 - precision: 0.9354 - recall: 0.8719 - tn: 2683314.7500 - tp: 261739.3281 - val_accuracy: 0.9028 - val_auc: 0.9917 - val_fn: 1159.0000 - val_fp: 636.0000 - val_loss: 0.3196 - val_precision: 0.9329 - val_recall: 0.8841 - val_tn: 89364.0000 - val_tp: 8841.0000 - learning_rate: 0.0010\n",
      "Epoch 154/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 18ms/step - accuracy: 0.9017 - auc: 0.9924 - fn: 38291.4531 - fp: 18190.1680 - loss: 0.3180 - precision: 0.9350 - recall: 0.8724 - tn: 2683249.2500 - tp: 261868.4844 - val_accuracy: 0.9032 - val_auc: 0.9917 - val_fn: 1152.0000 - val_fp: 632.0000 - val_loss: 0.3198 - val_precision: 0.9333 - val_recall: 0.8848 - val_tn: 89368.0000 - val_tp: 8848.0000 - learning_rate: 0.0010\n",
      "Epoch 155/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 15ms/step - accuracy: 0.9015 - auc: 0.9924 - fn: 38195.6484 - fp: 18142.1172 - loss: 0.3181 - precision: 0.9353 - recall: 0.8727 - tn: 2683297.5000 - tp: 261964.2969 - val_accuracy: 0.9036 - val_auc: 0.9917 - val_fn: 1145.0000 - val_fp: 637.0000 - val_loss: 0.3185 - val_precision: 0.9329 - val_recall: 0.8855 - val_tn: 89363.0000 - val_tp: 8855.0000 - learning_rate: 0.0010\n",
      "Epoch 156/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 18ms/step - accuracy: 0.9025 - auc: 0.9924 - fn: 37928.7148 - fp: 18023.7207 - loss: 0.3172 - precision: 0.9357 - recall: 0.8736 - tn: 2683415.7500 - tp: 262231.2188 - val_accuracy: 0.9033 - val_auc: 0.9918 - val_fn: 1144.0000 - val_fp: 631.0000 - val_loss: 0.3182 - val_precision: 0.9335 - val_recall: 0.8856 - val_tn: 89369.0000 - val_tp: 8856.0000 - learning_rate: 0.0010\n",
      "Epoch 157/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 17ms/step - accuracy: 0.9022 - auc: 0.9924 - fn: 38251.8086 - fp: 18270.0312 - loss: 0.3169 - precision: 0.9347 - recall: 0.8725 - tn: 2683169.5000 - tp: 261908.1406 - val_accuracy: 0.9035 - val_auc: 0.9918 - val_fn: 1145.0000 - val_fp: 636.0000 - val_loss: 0.3169 - val_precision: 0.9330 - val_recall: 0.8855 - val_tn: 89364.0000 - val_tp: 8855.0000 - learning_rate: 0.0010\n",
      "Epoch 158/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 17ms/step - accuracy: 0.9026 - auc: 0.9924 - fn: 38038.1797 - fp: 18005.3008 - loss: 0.3174 - precision: 0.9357 - recall: 0.8730 - tn: 2683434.2500 - tp: 262121.7656 - val_accuracy: 0.9044 - val_auc: 0.9919 - val_fn: 1139.0000 - val_fp: 633.0000 - val_loss: 0.3165 - val_precision: 0.9333 - val_recall: 0.8861 - val_tn: 89367.0000 - val_tp: 8861.0000 - learning_rate: 0.0010\n",
      "Epoch 159/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 18ms/step - accuracy: 0.9034 - auc: 0.9925 - fn: 37710.8672 - fp: 17994.2969 - loss: 0.3145 - precision: 0.9358 - recall: 0.8743 - tn: 2683445.2500 - tp: 262449.0625 - val_accuracy: 0.9040 - val_auc: 0.9919 - val_fn: 1133.0000 - val_fp: 627.0000 - val_loss: 0.3153 - val_precision: 0.9340 - val_recall: 0.8867 - val_tn: 89373.0000 - val_tp: 8867.0000 - learning_rate: 0.0010\n",
      "Epoch 160/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 16ms/step - accuracy: 0.9031 - auc: 0.9924 - fn: 37698.4883 - fp: 17966.1719 - loss: 0.3150 - precision: 0.9359 - recall: 0.8743 - tn: 2683473.2500 - tp: 262461.4688 - val_accuracy: 0.9042 - val_auc: 0.9918 - val_fn: 1133.0000 - val_fp: 634.0000 - val_loss: 0.3157 - val_precision: 0.9333 - val_recall: 0.8867 - val_tn: 89366.0000 - val_tp: 8867.0000 - learning_rate: 0.0010\n",
      "Epoch 161/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 17ms/step - accuracy: 0.9040 - auc: 0.9925 - fn: 37412.9023 - fp: 17857.1562 - loss: 0.3136 - precision: 0.9364 - recall: 0.8756 - tn: 2683582.2500 - tp: 262747.0312 - val_accuracy: 0.9038 - val_auc: 0.9918 - val_fn: 1132.0000 - val_fp: 631.0000 - val_loss: 0.3153 - val_precision: 0.9336 - val_recall: 0.8868 - val_tn: 89369.0000 - val_tp: 8868.0000 - learning_rate: 0.0010\n",
      "Epoch 162/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 21ms/step - accuracy: 0.9037 - auc: 0.9924 - fn: 37595.7617 - fp: 17912.9668 - loss: 0.3141 - precision: 0.9361 - recall: 0.8749 - tn: 2683526.5000 - tp: 262564.1875 - val_accuracy: 0.9046 - val_auc: 0.9919 - val_fn: 1127.0000 - val_fp: 628.0000 - val_loss: 0.3139 - val_precision: 0.9339 - val_recall: 0.8873 - val_tn: 89372.0000 - val_tp: 8873.0000 - learning_rate: 0.0010\n",
      "Epoch 163/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 21ms/step - accuracy: 0.9045 - auc: 0.9926 - fn: 37358.2773 - fp: 17758.1387 - loss: 0.3113 - precision: 0.9367 - recall: 0.8755 - tn: 2683681.2500 - tp: 262801.6562 - val_accuracy: 0.9045 - val_auc: 0.9919 - val_fn: 1122.0000 - val_fp: 629.0000 - val_loss: 0.3132 - val_precision: 0.9338 - val_recall: 0.8878 - val_tn: 89371.0000 - val_tp: 8878.0000 - learning_rate: 0.0010\n",
      "Epoch 164/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 18ms/step - accuracy: 0.9043 - auc: 0.9925 - fn: 37223.0820 - fp: 17612.1836 - loss: 0.3115 - precision: 0.9375 - recall: 0.8758 - tn: 2683827.2500 - tp: 262936.8750 - val_accuracy: 0.9045 - val_auc: 0.9919 - val_fn: 1121.0000 - val_fp: 629.0000 - val_loss: 0.3124 - val_precision: 0.9338 - val_recall: 0.8879 - val_tn: 89371.0000 - val_tp: 8879.0000 - learning_rate: 0.0010\n",
      "Epoch 165/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 15ms/step - accuracy: 0.9046 - auc: 0.9926 - fn: 37078.8359 - fp: 17742.5566 - loss: 0.3096 - precision: 0.9369 - recall: 0.8765 - tn: 2683697.0000 - tp: 263081.0938 - val_accuracy: 0.9046 - val_auc: 0.9919 - val_fn: 1119.0000 - val_fp: 633.0000 - val_loss: 0.3129 - val_precision: 0.9335 - val_recall: 0.8881 - val_tn: 89367.0000 - val_tp: 8881.0000 - learning_rate: 0.0010\n",
      "Epoch 166/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 16ms/step - accuracy: 0.9050 - auc: 0.9926 - fn: 37123.0586 - fp: 17736.6895 - loss: 0.3099 - precision: 0.9371 - recall: 0.8764 - tn: 2683702.7500 - tp: 263036.8750 - val_accuracy: 0.9047 - val_auc: 0.9919 - val_fn: 1124.0000 - val_fp: 629.0000 - val_loss: 0.3131 - val_precision: 0.9338 - val_recall: 0.8876 - val_tn: 89371.0000 - val_tp: 8876.0000 - learning_rate: 0.0010\n",
      "Epoch 167/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 15ms/step - accuracy: 0.9043 - auc: 0.9925 - fn: 37122.9648 - fp: 17783.0547 - loss: 0.3110 - precision: 0.9364 - recall: 0.8759 - tn: 2683656.5000 - tp: 263036.9688 - val_accuracy: 0.9045 - val_auc: 0.9919 - val_fn: 1118.0000 - val_fp: 635.0000 - val_loss: 0.3121 - val_precision: 0.9333 - val_recall: 0.8882 - val_tn: 89365.0000 - val_tp: 8882.0000 - learning_rate: 0.0010\n",
      "Epoch 168/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 17ms/step - accuracy: 0.9046 - auc: 0.9927 - fn: 37141.3281 - fp: 17828.9414 - loss: 0.3088 - precision: 0.9363 - recall: 0.8759 - tn: 2683610.5000 - tp: 263018.6250 - val_accuracy: 0.9051 - val_auc: 0.9919 - val_fn: 1121.0000 - val_fp: 628.0000 - val_loss: 0.3118 - val_precision: 0.9339 - val_recall: 0.8879 - val_tn: 89372.0000 - val_tp: 8879.0000 - learning_rate: 0.0010\n",
      "Epoch 169/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 19ms/step - accuracy: 0.9049 - auc: 0.9928 - fn: 36972.9414 - fp: 17804.9648 - loss: 0.3080 - precision: 0.9367 - recall: 0.8769 - tn: 2683634.5000 - tp: 263187.0000 - val_accuracy: 0.9051 - val_auc: 0.9920 - val_fn: 1112.0000 - val_fp: 628.0000 - val_loss: 0.3109 - val_precision: 0.9340 - val_recall: 0.8888 - val_tn: 89372.0000 - val_tp: 8888.0000 - learning_rate: 0.0010\n",
      "Epoch 170/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 16ms/step - accuracy: 0.9055 - auc: 0.9927 - fn: 36813.0039 - fp: 17609.9707 - loss: 0.3075 - precision: 0.9373 - recall: 0.8771 - tn: 2683829.5000 - tp: 263346.9375 - val_accuracy: 0.9054 - val_auc: 0.9920 - val_fn: 1114.0000 - val_fp: 623.0000 - val_loss: 0.3101 - val_precision: 0.9345 - val_recall: 0.8886 - val_tn: 89377.0000 - val_tp: 8886.0000 - learning_rate: 0.0010\n",
      "Epoch 171/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 17ms/step - accuracy: 0.9062 - auc: 0.9928 - fn: 36770.3906 - fp: 17548.0000 - loss: 0.3054 - precision: 0.9375 - recall: 0.8776 - tn: 2683891.5000 - tp: 263389.5625 - val_accuracy: 0.9049 - val_auc: 0.9919 - val_fn: 1114.0000 - val_fp: 632.0000 - val_loss: 0.3104 - val_precision: 0.9336 - val_recall: 0.8886 - val_tn: 89368.0000 - val_tp: 8886.0000 - learning_rate: 0.0010\n",
      "Epoch 172/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 19ms/step - accuracy: 0.9064 - auc: 0.9928 - fn: 36532.1797 - fp: 17558.7305 - loss: 0.3048 - precision: 0.9376 - recall: 0.8785 - tn: 2683880.7500 - tp: 263627.7812 - val_accuracy: 0.9057 - val_auc: 0.9920 - val_fn: 1111.0000 - val_fp: 634.0000 - val_loss: 0.3105 - val_precision: 0.9334 - val_recall: 0.8889 - val_tn: 89366.0000 - val_tp: 8889.0000 - learning_rate: 0.0010\n",
      "Epoch 173/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 21ms/step - accuracy: 0.9054 - auc: 0.9929 - fn: 36593.0195 - fp: 17593.4258 - loss: 0.3057 - precision: 0.9374 - recall: 0.8779 - tn: 2683846.0000 - tp: 263566.9375 - val_accuracy: 0.9062 - val_auc: 0.9920 - val_fn: 1109.0000 - val_fp: 623.0000 - val_loss: 0.3092 - val_precision: 0.9345 - val_recall: 0.8891 - val_tn: 89377.0000 - val_tp: 8891.0000 - learning_rate: 0.0010\n",
      "Epoch 174/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 16ms/step - accuracy: 0.9065 - auc: 0.9928 - fn: 36563.6680 - fp: 17667.6660 - loss: 0.3051 - precision: 0.9373 - recall: 0.8783 - tn: 2683771.7500 - tp: 263596.2812 - val_accuracy: 0.9061 - val_auc: 0.9921 - val_fn: 1105.0000 - val_fp: 630.0000 - val_loss: 0.3089 - val_precision: 0.9339 - val_recall: 0.8895 - val_tn: 89370.0000 - val_tp: 8895.0000 - learning_rate: 0.0010\n",
      "Epoch 175/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 25ms/step - accuracy: 0.9064 - auc: 0.9929 - fn: 36337.3320 - fp: 17577.2910 - loss: 0.3044 - precision: 0.9374 - recall: 0.8786 - tn: 2683862.2500 - tp: 263822.6250 - val_accuracy: 0.9060 - val_auc: 0.9920 - val_fn: 1101.0000 - val_fp: 625.0000 - val_loss: 0.3092 - val_precision: 0.9344 - val_recall: 0.8899 - val_tn: 89375.0000 - val_tp: 8899.0000 - learning_rate: 0.0010\n",
      "Epoch 176/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 16ms/step - accuracy: 0.9066 - auc: 0.9928 - fn: 36507.9102 - fp: 17614.5176 - loss: 0.3043 - precision: 0.9376 - recall: 0.8788 - tn: 2683825.0000 - tp: 263652.0312 - val_accuracy: 0.9065 - val_auc: 0.9921 - val_fn: 1098.0000 - val_fp: 621.0000 - val_loss: 0.3072 - val_precision: 0.9348 - val_recall: 0.8902 - val_tn: 89379.0000 - val_tp: 8902.0000 - learning_rate: 0.0010\n",
      "Epoch 177/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 16ms/step - accuracy: 0.9073 - auc: 0.9929 - fn: 36088.2500 - fp: 17405.1504 - loss: 0.3025 - precision: 0.9384 - recall: 0.8801 - tn: 2684034.2500 - tp: 264071.6875 - val_accuracy: 0.9071 - val_auc: 0.9921 - val_fn: 1093.0000 - val_fp: 623.0000 - val_loss: 0.3058 - val_precision: 0.9346 - val_recall: 0.8907 - val_tn: 89377.0000 - val_tp: 8907.0000 - learning_rate: 0.0010\n",
      "Epoch 178/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 20ms/step - accuracy: 0.9073 - auc: 0.9928 - fn: 36066.4102 - fp: 17415.1816 - loss: 0.3021 - precision: 0.9381 - recall: 0.8799 - tn: 2684024.2500 - tp: 264093.5312 - val_accuracy: 0.9065 - val_auc: 0.9920 - val_fn: 1095.0000 - val_fp: 626.0000 - val_loss: 0.3073 - val_precision: 0.9343 - val_recall: 0.8905 - val_tn: 89374.0000 - val_tp: 8905.0000 - learning_rate: 0.0010\n",
      "Epoch 179/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 19ms/step - accuracy: 0.9068 - auc: 0.9929 - fn: 36190.7031 - fp: 17530.3691 - loss: 0.3034 - precision: 0.9377 - recall: 0.8793 - tn: 2683909.2500 - tp: 263969.2500 - val_accuracy: 0.9076 - val_auc: 0.9921 - val_fn: 1084.0000 - val_fp: 622.0000 - val_loss: 0.3052 - val_precision: 0.9348 - val_recall: 0.8916 - val_tn: 89378.0000 - val_tp: 8916.0000 - learning_rate: 0.0010\n",
      "Epoch 180/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 15ms/step - accuracy: 0.9075 - auc: 0.9929 - fn: 35932.1133 - fp: 17391.9863 - loss: 0.3013 - precision: 0.9384 - recall: 0.8802 - tn: 2684047.5000 - tp: 264227.8438 - val_accuracy: 0.9078 - val_auc: 0.9921 - val_fn: 1086.0000 - val_fp: 625.0000 - val_loss: 0.3058 - val_precision: 0.9345 - val_recall: 0.8914 - val_tn: 89375.0000 - val_tp: 8914.0000 - learning_rate: 0.0010\n",
      "Epoch 181/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 23ms/step - accuracy: 0.9077 - auc: 0.9930 - fn: 36040.7461 - fp: 17402.0918 - loss: 0.3004 - precision: 0.9382 - recall: 0.8801 - tn: 2684037.5000 - tp: 264119.1875 - val_accuracy: 0.9083 - val_auc: 0.9922 - val_fn: 1083.0000 - val_fp: 617.0000 - val_loss: 0.3047 - val_precision: 0.9353 - val_recall: 0.8917 - val_tn: 89383.0000 - val_tp: 8917.0000 - learning_rate: 0.0010\n",
      "Epoch 182/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 19ms/step - accuracy: 0.9077 - auc: 0.9930 - fn: 35884.7305 - fp: 17322.0605 - loss: 0.2995 - precision: 0.9385 - recall: 0.8803 - tn: 2684117.5000 - tp: 264275.2188 - val_accuracy: 0.9085 - val_auc: 0.9922 - val_fn: 1074.0000 - val_fp: 622.0000 - val_loss: 0.3036 - val_precision: 0.9349 - val_recall: 0.8926 - val_tn: 89378.0000 - val_tp: 8926.0000 - learning_rate: 0.0010\n",
      "Epoch 183/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 16ms/step - accuracy: 0.9073 - auc: 0.9930 - fn: 36042.7656 - fp: 17333.7363 - loss: 0.3005 - precision: 0.9383 - recall: 0.8799 - tn: 2684105.7500 - tp: 264117.1875 - val_accuracy: 0.9083 - val_auc: 0.9921 - val_fn: 1071.0000 - val_fp: 624.0000 - val_loss: 0.3042 - val_precision: 0.9347 - val_recall: 0.8929 - val_tn: 89376.0000 - val_tp: 8929.0000 - learning_rate: 0.0010\n",
      "Epoch 184/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 14ms/step - accuracy: 0.9081 - auc: 0.9931 - fn: 35703.3477 - fp: 17324.4824 - loss: 0.2990 - precision: 0.9386 - recall: 0.8809 - tn: 2684115.0000 - tp: 264456.5938 - val_accuracy: 0.9086 - val_auc: 0.9921 - val_fn: 1074.0000 - val_fp: 622.0000 - val_loss: 0.3037 - val_precision: 0.9349 - val_recall: 0.8926 - val_tn: 89378.0000 - val_tp: 8926.0000 - learning_rate: 0.0010\n",
      "Epoch 185/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 14ms/step - accuracy: 0.9087 - auc: 0.9930 - fn: 35592.5156 - fp: 17239.1230 - loss: 0.2979 - precision: 0.9391 - recall: 0.8816 - tn: 2684200.5000 - tp: 264567.4375 - val_accuracy: 0.9085 - val_auc: 0.9920 - val_fn: 1070.0000 - val_fp: 623.0000 - val_loss: 0.3033 - val_precision: 0.9348 - val_recall: 0.8930 - val_tn: 89377.0000 - val_tp: 8930.0000 - learning_rate: 0.0010\n",
      "Epoch 186/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 16ms/step - accuracy: 0.9087 - auc: 0.9930 - fn: 35559.9961 - fp: 17338.0566 - loss: 0.2976 - precision: 0.9385 - recall: 0.8818 - tn: 2684101.5000 - tp: 264599.9375 - val_accuracy: 0.9087 - val_auc: 0.9921 - val_fn: 1068.0000 - val_fp: 618.0000 - val_loss: 0.3035 - val_precision: 0.9353 - val_recall: 0.8932 - val_tn: 89382.0000 - val_tp: 8932.0000 - learning_rate: 0.0010\n",
      "Epoch 187/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 21ms/step - accuracy: 0.9084 - auc: 0.9931 - fn: 35615.0508 - fp: 17210.2793 - loss: 0.2975 - precision: 0.9388 - recall: 0.8813 - tn: 2684229.2500 - tp: 264544.9062 - val_accuracy: 0.9092 - val_auc: 0.9921 - val_fn: 1064.0000 - val_fp: 620.0000 - val_loss: 0.3026 - val_precision: 0.9351 - val_recall: 0.8936 - val_tn: 89380.0000 - val_tp: 8936.0000 - learning_rate: 0.0010\n",
      "Epoch 188/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 22ms/step - accuracy: 0.9090 - auc: 0.9931 - fn: 35358.9062 - fp: 17227.6328 - loss: 0.2973 - precision: 0.9390 - recall: 0.8822 - tn: 2684212.0000 - tp: 264801.0312 - val_accuracy: 0.9088 - val_auc: 0.9921 - val_fn: 1057.0000 - val_fp: 622.0000 - val_loss: 0.3023 - val_precision: 0.9350 - val_recall: 0.8943 - val_tn: 89378.0000 - val_tp: 8943.0000 - learning_rate: 0.0010\n",
      "Epoch 189/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 22ms/step - accuracy: 0.9088 - auc: 0.9931 - fn: 35336.0117 - fp: 17148.4824 - loss: 0.2972 - precision: 0.9391 - recall: 0.8820 - tn: 2684291.0000 - tp: 264823.9375 - val_accuracy: 0.9089 - val_auc: 0.9921 - val_fn: 1060.0000 - val_fp: 620.0000 - val_loss: 0.3016 - val_precision: 0.9351 - val_recall: 0.8940 - val_tn: 89380.0000 - val_tp: 8940.0000 - learning_rate: 0.0010\n",
      "Epoch 190/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 17ms/step - accuracy: 0.9091 - auc: 0.9931 - fn: 35299.5195 - fp: 17120.0723 - loss: 0.2963 - precision: 0.9395 - recall: 0.8823 - tn: 2684319.5000 - tp: 264860.4375 - val_accuracy: 0.9089 - val_auc: 0.9921 - val_fn: 1053.0000 - val_fp: 622.0000 - val_loss: 0.3010 - val_precision: 0.9350 - val_recall: 0.8947 - val_tn: 89378.0000 - val_tp: 8947.0000 - learning_rate: 0.0010\n",
      "Epoch 191/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 17ms/step - accuracy: 0.9090 - auc: 0.9931 - fn: 35325.1172 - fp: 17127.7148 - loss: 0.2952 - precision: 0.9391 - recall: 0.8822 - tn: 2684311.7500 - tp: 264834.8438 - val_accuracy: 0.9092 - val_auc: 0.9922 - val_fn: 1058.0000 - val_fp: 622.0000 - val_loss: 0.3012 - val_precision: 0.9350 - val_recall: 0.8942 - val_tn: 89378.0000 - val_tp: 8942.0000 - learning_rate: 0.0010\n",
      "Epoch 192/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 16ms/step - accuracy: 0.9098 - auc: 0.9932 - fn: 35095.6211 - fp: 17134.6387 - loss: 0.2939 - precision: 0.9395 - recall: 0.8835 - tn: 2684304.7500 - tp: 265064.3125 - val_accuracy: 0.9094 - val_auc: 0.9921 - val_fn: 1045.0000 - val_fp: 621.0000 - val_loss: 0.3010 - val_precision: 0.9352 - val_recall: 0.8955 - val_tn: 89379.0000 - val_tp: 8955.0000 - learning_rate: 0.0010\n",
      "Epoch 193/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 23ms/step - accuracy: 0.9096 - auc: 0.9931 - fn: 34987.8438 - fp: 17071.6641 - loss: 0.2949 - precision: 0.9395 - recall: 0.8831 - tn: 2684367.7500 - tp: 265172.0938 - val_accuracy: 0.9099 - val_auc: 0.9921 - val_fn: 1041.0000 - val_fp: 613.0000 - val_loss: 0.2993 - val_precision: 0.9360 - val_recall: 0.8959 - val_tn: 89387.0000 - val_tp: 8959.0000 - learning_rate: 0.0010\n",
      "Epoch 194/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 13ms/step - accuracy: 0.9102 - auc: 0.9932 - fn: 34868.2383 - fp: 16964.1777 - loss: 0.2933 - precision: 0.9398 - recall: 0.8838 - tn: 2684475.2500 - tp: 265291.7188 - val_accuracy: 0.9102 - val_auc: 0.9921 - val_fn: 1039.0000 - val_fp: 617.0000 - val_loss: 0.2996 - val_precision: 0.9356 - val_recall: 0.8961 - val_tn: 89383.0000 - val_tp: 8961.0000 - learning_rate: 0.0010\n",
      "Epoch 195/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 12ms/step - accuracy: 0.9103 - auc: 0.9933 - fn: 34758.8438 - fp: 17030.3633 - loss: 0.2928 - precision: 0.9396 - recall: 0.8843 - tn: 2684409.2500 - tp: 265401.0938 - val_accuracy: 0.9103 - val_auc: 0.9921 - val_fn: 1045.0000 - val_fp: 618.0000 - val_loss: 0.2995 - val_precision: 0.9354 - val_recall: 0.8955 - val_tn: 89382.0000 - val_tp: 8955.0000 - learning_rate: 0.0010\n",
      "Epoch 196/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 14ms/step - accuracy: 0.9105 - auc: 0.9932 - fn: 34815.2852 - fp: 16935.5195 - loss: 0.2922 - precision: 0.9402 - recall: 0.8843 - tn: 2684504.0000 - tp: 265344.6562 - val_accuracy: 0.9105 - val_auc: 0.9921 - val_fn: 1039.0000 - val_fp: 619.0000 - val_loss: 0.2995 - val_precision: 0.9354 - val_recall: 0.8961 - val_tn: 89381.0000 - val_tp: 8961.0000 - learning_rate: 0.0010\n",
      "Epoch 197/1000\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 19ms/step - accuracy: 0.9101 - auc: 0.9932 - fn: 34892.7930 - fp: 16968.3105 - loss: 0.2925 - precision: 0.9401 - recall: 0.8838 - tn: 2684471.2500 - tp: 265267.1562 - val_accuracy: 0.9107 - val_auc: 0.9921 - val_fn: 1035.0000 - val_fp: 614.0000 - val_loss: 0.2989 - val_precision: 0.9359 - val_recall: 0.8965 - val_tn: 89386.0000 - val_tp: 8965.0000 - learning_rate: 0.0010\n",
      "Epoch 198/1000\n",
      "\u001b[1m3396/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 19ms/step - accuracy: 0.9105 - auc: 0.9933 - fn: 25282.0820 - fp: 12398.0889 - loss: 0.2924 - precision: 0.9392 - recall: 0.8836 - tn: 1944273.8750 - tp: 192125.9219"
     ]
    }
   ],
   "source": [
    "# callbacks cnn\n",
    "callbacks_cnn_aug = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10),\n",
    "    ModelCheckpoint(PATH_MODELS + f'best_model_CNN_aug.keras', save_best_only=True, monitor='val_loss'),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5),\n",
    "    TensorBoard(log_dir=PATH_LOGS),\n",
    "    CSVLogger(PATH_MODELS + f'training_log_CNN_aug.csv')\n",
    "]\n",
    "\n",
    "# limpa sessao do keras\n",
    "keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "# cria modelo\n",
    "model_cnn_aug = create_cnn_model(\n",
    "    input_shape=input_shape,\n",
    "    num_classes= num_classes,\n",
    "    metrics = METRICS\n",
    ")\n",
    "\n",
    "# treina modelo\n",
    "history_cnn_aug = model_cnn_aug.fit(X_train_augmented, y_train_augmented,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test),\n",
    "          callbacks=callbacks_cnn_aug\n",
    "          )\n",
    "\n",
    "\n",
    "# realiza predicao\n",
    "train_pred_proba = model_cnn_aug.predict(X_train)\n",
    "test_pred_proba = model_cnn_aug.predict(X_test)\n",
    "\n",
    "# salva predicao no treino\n",
    "test_results_cnn_aug = pd.DataFrame({\n",
    "    \"y_test\": y_test.argmax(axis=1),\n",
    "    \"y_test_pred\": np.argmax(test_pred_proba, axis =1),\n",
    "    \"y_test_pred_proba\": list(test_pred_proba)\n",
    "    \n",
    "})\n",
    "test_results_cnn_aug.to_csv(RESULTS_PATH + 'test/cnn_aug_test_results.csv', index = False)\n",
    "\n",
    "# salva predicao no teste\n",
    "train_results_cnn_aug = pd.DataFrame({\n",
    "    \"y_train\": y_train.argmax(axis=1),\n",
    "    \"y_train_pred\": np.argmax(train_pred_proba, axis =1),\n",
    "    \"y_train_pred_proba\": list(train_pred_proba)\n",
    "})\n",
    "train_results_cnn_aug.to_csv(RESULTS_PATH + 'train/cnn_aug_train_results.csv', index = False)\n",
    "\n",
    "\n",
    "json.dump(history_cnn_aug.history, open(RESULTS_PATH + \"history/cnn_aug_history.json\", \"w\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks cnn\n",
    "callbacks_fcnn = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10),\n",
    "    ModelCheckpoint(PATH_MODELS + f'best_model_fcnn.keras', save_best_only=True, monitor='val_loss'),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5),\n",
    "    TensorBoard(log_dir=PATH_LOGS),\n",
    "    CSVLogger(PATH_MODELS + f'training_log_fcnn.csv')\n",
    "]\n",
    "\n",
    "# limpa sessao do keras\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# cria modelo\n",
    "model_fcnn = create_fcnn_model(\n",
    "    input_shape=input_shape,\n",
    "    num_classes= num_classes,\n",
    "    metrics = METRICS\n",
    ")\n",
    "\n",
    "# treina modelo\n",
    "history_fcnn = model_fcnn.fit(X_train, y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test),\n",
    "          callbacks=callbacks_fcnn\n",
    "          )\n",
    "\n",
    "\n",
    "# realiza predicao\n",
    "train_pred_proba = model_fcnn.predict(X_train)\n",
    "test_pred_proba = model_fcnn.predict(X_test)\n",
    "\n",
    "# salva predicao no treino\n",
    "test_results_fcnn = pd.DataFrame({\n",
    "    \"y_test\": y_test.argmax(axis=1),\n",
    "    \"y_test_pred\": np.argmax(test_pred_proba, axis =1),\n",
    "    \"y_test_pred_proba\": list(test_pred_proba)\n",
    "    \n",
    "})\n",
    "test_results_fcnn.to_csv(RESULTS_PATH + 'test/fcnn_test_results.csv', index = False)\n",
    "\n",
    "\n",
    "# salva predicao no teste\n",
    "train_results_fcnn = pd.DataFrame({\n",
    "    \"y_train\": y_train.argmax(axis=1),\n",
    "    \"y_train_pred\": np.argmax(train_pred_proba, axis =1),\n",
    "    \"y_train_pred_proba\": list(train_pred_proba)\n",
    "})\n",
    "train_results_fcnn.to_csv(RESULTS_PATH + 'train/fcnn_train_results.csv', index = False)\n",
    "\n",
    "\n",
    "json.dump(history_fcnn.history, open(RESULTS_PATH + \"history/fcnn_history.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### com data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# callbacks cnn\n",
    "callbacks_fcnn_aug = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10),\n",
    "    ModelCheckpoint(PATH_MODELS + f'best_model_fcnn_aug.keras', save_best_only=True, monitor='val_loss'),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5),\n",
    "    TensorBoard(log_dir=PATH_LOGS),\n",
    "    CSVLogger(PATH_MODELS + f'training_log_fcnn_aug.csv')\n",
    "]\n",
    "\n",
    "# limpa sessao do keras\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# cria modelo\n",
    "model_fcnn_aug = create_fcnn_model(\n",
    "    input_shape=input_shape,\n",
    "    num_classes= num_classes,\n",
    "    metrics = METRICS\n",
    ")\n",
    "\n",
    "# treina modelo\n",
    "history_fcnn_aug = model_fcnn_aug.fit(X_train, y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test),\n",
    "          callbacks=callbacks_fcnn_aug\n",
    "          )\n",
    "\n",
    "\n",
    "# realiza predicao\n",
    "train_pred_proba = model_fcnn_aug.predict(X_train)\n",
    "test_pred_proba = model_fcnn_aug.predict(X_test)\n",
    "\n",
    "# salva predicao no treino\n",
    "test_results_fcnn_aug = pd.DataFrame({\n",
    "    \"y_test\": y_test.argmax(axis=1),\n",
    "    \"y_test_pred\": np.argmax(test_pred_proba, axis =1),\n",
    "    \"y_test_pred_proba\": list(test_pred_proba)\n",
    "    \n",
    "})\n",
    "test_results_fcnn_aug.to_csv(RESULTS_PATH + 'test/fcnn_aug_test_results.csv', index = False)\n",
    "\n",
    "\n",
    "# salva predicao no teste\n",
    "train_results_fcnn_aug = pd.DataFrame({\n",
    "    \"y_train\": y_train.argmax(axis=1),\n",
    "    \"y_train_pred\": np.argmax(train_pred_proba, axis =1),\n",
    "    \"y_train_pred_proba\": list(train_pred_proba)\n",
    "})\n",
    "train_results_fcnn_aug.to_csv(RESULTS_PATH + 'train/fcnn_aug_train_results.csv', index = False)\n",
    "\n",
    "\n",
    "json.dump(history_fcnn_aug.history, open(RESULTS_PATH + \"history/fcnn_aug_history.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-kmnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
